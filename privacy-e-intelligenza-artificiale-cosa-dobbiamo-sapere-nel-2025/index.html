<!DOCTYPE html><html lang="it"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Privacy e intelligenza artificiale: cosa dobbiamo sapere nel 2025 - pianetararo</title><meta name="description" content="Cosa devi sapere su privacy e intelligenza artificiale nel 2025: dai rischi di ChatGPT alle sanzioni GDPR, tutto quello che serve per proteggerti."><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://pianetararo.org/privacy-e-intelligenza-artificiale-cosa-dobbiamo-sapere-nel-2025/"><link rel="alternate" type="application/atom+xml" href="https://pianetararo.org/feed.xml"><link rel="alternate" type="application/json" href="https://pianetararo.org/feed.json"><meta property="og:title" content="Privacy e intelligenza artificiale: cosa dobbiamo sapere nel 2025"><meta property="og:image" content="https://pianetararo.org/media/posts/21/g9ddxhg9ddxhg9dd.jfif"><meta property="og:image:width" content="2816"><meta property="og:image:height" content="1536"><meta property="og:site_name" content="pianetararo"><meta property="og:description" content="Cosa devi sapere su privacy e intelligenza artificiale nel 2025: dai rischi di ChatGPT alle sanzioni GDPR, tutto quello che serve per proteggerti."><meta property="og:url" content="https://pianetararo.org//privacy-e-intelligenza-artificiale-cosa-dobbiamo-sapere-nel-2025/"><meta property="og:type" content="article"><link rel="stylesheet" href="https://pianetararo.org/assets/css/style.css?v=812e0178178abea4ea9399c6007c2ff4"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://pianetararo.org/privacy-e-intelligenza-artificiale-cosa-dobbiamo-sapere-nel-2025/"},"headline":"Privacy e intelligenza artificiale: cosa dobbiamo sapere nel 2025","datePublished":"2025-04-13T20:49+02:00","dateModified":"2025-06-13T12:02+02:00","image":{"@type":"ImageObject","url":"https://pianetararo.org/media/posts/21/g9ddxhg9ddxhg9dd.jfif","height":1536,"width":2816},"description":"Cosa devi sapere su privacy e intelligenza artificiale nel 2025: dai rischi di ChatGPT alle sanzioni GDPR, tutto quello che serve per proteggerti.","author":{"@type":"Person","name":"Pianetararo Associazione Culturale","url":"https://pianetararo.org/authors/pianetararo-associazione-culturale/"},"publisher":{"@type":"Organization","name":"Pianetararo Associazione Culturale","logo":{"@type":"ImageObject","url":"https://pianetararo.org/media/website/PIANETARARO-1-1-1.svg","height":375,"width":375}}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><header class="top js-header"><a class="logo" href="https://pianetararo.org/"><img src="https://pianetararo.org/media/website/PIANETARARO-1-1-1.svg" alt="pianetararo" width="375" height="375"></a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://pianetararo.org/test-2/" title="pianetararo" target="_self">Chi siamo</a></li><li><a href="https://pianetararo.org/traiettorie/" target="_self">TrAIettorie</a></li><li class="has-submenu"><a href="https://pianetararo.org/frammenti25intro/" title="frammenti25" target="_self" aria-haspopup="true">FRAMMENTI#25</a><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/" target="_self">FRAMMENTI DIGITAL - Come funziona ?</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/1-il-pozzo-di-san-patrizio/" target="_self">#1 - Gennaio</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/2-la-rocca-di-calascio/" target="_self">#2 - Febbraio</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/3-il-palio-delle-rane/" target="_self">#3 - Marzo</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/4-il-carnevale-di-mamoiada/" target="_self">#4 - Aprile</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/5-il-castello-di-montebello-e-la-dama-bianca/" target="_self">#5 - Maggio</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/6/" target="_self">#6 - Giugno</a></li></ul></li><li><a href="https://pianetararo.org/stradora/" title="STRADORA" target="_self">STRADORA</a></li><li><a href="https://pianetararo.org/pensieri/" title="PENSIERI" target="_self">Pensieri</a></li><li><a href="https://pianetararo.org/tags/" target="_self">Tags</a></li></ul></nav></header><main class="post"><article class="content"><div class="hero"><header class="hero__content"><div class="wrapper"><h1>Privacy e intelligenza artificiale: cosa dobbiamo sapere nel 2025</h1><div class="feed__meta content__meta"><time datetime="2025-04-13T20:49" class="feed__date">13 apr 2025</time></div></div></header><figure class="hero__image"><div class="hero__image-wrapper"><img src="https://pianetararo.org/media/posts/21/g9ddxhg9ddxhg9dd.jfif" loading="eager" height="1536" width="2816" alt=""></div></figure></div><div class="entry-wrapper content__entry"><p><span style="font-weight: 400;">Diciamocelo: ormai interagire con un chatbot – che sia ChatGPT, Gemini, Claude o uno dei mille assistenti virtuali – è diventato quasi come fare una chiacchierata al bar. Solo che, al posto del barista, c’è un algoritmo che ci risponde in un italiano perfetto (o quasi). Studenti, insegnanti, genitori, professionisti – chi più chi meno – ci siamo messi a “parlare” con queste AI per fare domande, chiedere aiuto, o semplicemente toglierci una curiosità. Ma sai cosa? Ogni volta che parliamo con un LLM – un modello linguistico generativo, per usare il termine tecnico – stiamo lasciando dietro di noi una scia di dati. Un po’ come se, dopo aver chiesto al barista un consiglio, scoprissimo che stava registrando la nostra conversazione per studiarci meglio.</span></p><p><span style="font-weight: 400;">E qui si apre un tema enorme: </span><strong>la privacy</strong><span style="font-weight: 400;">. Perché questi strumenti, sì, sono incredibili – ma a che prezzo? Quali dati cediamo, consapevoli o meno, quando chiediamo un consiglio per l’ansia, carichiamo la foto di famiglia su un generatore d’immagini, o raccontiamo una situazione privata al chatbot? Questo articolo (che sì, è lungo, ma merita) prova a fare chiarezza e stimolare qualche riflessione. Analizzeremo i rischi, le leggi in gioco – il </span><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32016R0679"><span style="font-weight: 400;">GDPR</span></a><span style="font-weight: 400;">, l’</span><a href="https://artificialintelligenceact.eu/"><span style="font-weight: 400;">AI Act</span></a><span style="font-weight: 400;">  – e ci faremo anche qualche domanda scomoda: </span><strong>cosa fanno le Big Tech con i nostri dati?</strong><span style="font-weight: 400;"> E noi, ci dobbiamo preoccupare?</span></p><p><span style="font-weight: 400;">Prima però, fermiamoci un attimo a guardare i rischi veri. Quelli meno ovvi. Quelli che spesso ignoriamo mentre chattiamo con l’IA pensando “tanto è solo un robot”.</span></p><p><span style="font-weight: 400;">Non è roba da smanettoni o da esperti: riguarda tutti noi. Studenti, insegnanti, genitori, professionisti, chiunque usi questi strumenti per lavorare, imparare o semplicemente divertirsi. Perché l’IA non è più “roba da laboratorio”: è parte della nostra vita di tutti i giorni. E capire come funziona – e come può influire sulla nostra privacy – è un modo per proteggere non solo i nostri dati, ma anche la nostra libertà di essere chi siamo, senza che un algoritmo ci metta in un’etichetta.</span></p><h2>I rischi principali: una panoramica pratica</h2><p><span style="font-weight: 400;">Partiamo dalle basi. Usare uno strumento di Intelligenza artificiale, per esempio un Large Language Model come ChatGPT, non è come scrivere sul quaderno: è come parlare in un microfono acceso, con qualcuno (o qualcosa) che sta ascoltando, registrando e, talvolta, imparando da quello che diciamo. Ecco una carrellata di rischi, spiegati in modo semplice – con esempi reali, perché non stiamo parlando di teorie campate in aria.</span></p><h5>⚠️ Condivisione involontaria di dati</h5><p><span style="font-weight: 400;">Quando scrivi “Ehi ChatGPT, dammi un consiglio per il compleanno del mio amico Marco, che soffre di ansia”? Ecco: in quel momento hai appena dato due informazioni personali – il nome di Marco e un dato sensibile sulla sua salute. Magari non ci hai fatto caso, ma il chatbot (e chi lo gestisce) le ha registrate. E potrebbero rimanere lì, nei log, anche per mesi.</span></p><h5>⚠️Prompt conservati e riutilizzati</h5><p><span style="font-weight: 400;">Le conversazioni con gli LLM non spariscono nel nulla. Spesso vengono archiviate e usate per “insegnare” al modello a migliorare. Quindi il tuo messaggio, la tua storia, le tue domande – potrebbero contribuire ad addestrare la prossima versione di quel chatbot. Se pensi che basti cancellare la cronologia per stare tranquilli, sappi che anche in quel caso i dati potrebbero rimanere salvati per 30 giorni (come succedeva e succede anche con alcuni piani con ChatGPT) oppure anche </span><a href="https://www.repubblica.it/tecnologia/2025/06/09/news/open-ai-obbligata-conservare-conversazioni-chatgpt-new-york-times-424657181/"><span style="font-weight: 400;">per sempre</span></a><span style="font-weight: 400;">.</span></p><h5>⚠️Occhi umani dietro le quinte</h5><p><span style="font-weight: 400;">Non c’è solo l’algoritmo: a volte, per migliorare l’IA, ci sono anche persone in carne e ossa che leggono pezzi delle conversazioni. Certo, si parla di chat “anonimizzate”, ma se scrivi “Mio figlio Lorenzo fa le medie a Roma e ha problemi con la matematica”, quei dati, anche senza il tuo nome, sono comunque riconoscibili. E magari finiscono davanti agli occhi di un revisore umano, chissà dove nel mondo.</span></p><h5>⚠️Tracciamento invisibile e fingerprinting</h5><p><span style="font-weight: 400;">Un po’ come quando navighi su un sito e ti senti “seguito” dagli annunci, anche qui i sistemi AI raccolgono informazioni di contorno: il tuo IP, il browser, il dispositivo, perfino quanto tempo rimani su una pagina o a che ora scrivi. Tutti dettagli che, messi insieme, costruiscono un profilo abbastanza dettagliato – e tu magari nemmeno te ne accorgi. È come lasciare briciole digitali che qualcun altro raccoglie.</span></p><h5>⚠️ Profilazione implicita</h5><p><span style="font-weight: 400;">Anche senza dirlo esplicitamente, il chatbot può “capire” cose su di te. Se gli chiedi spesso consigli su sintomi di malattie, oppure parli di viaggi in determinati Paesi, o di hobby particolari, il sistema può dedurre – e magari registrare – che sei interessato a quelle cose. E questo profilo, se finisce in mani sbagliate o viene usato per scopi di marketing, o peggio azioni di condizionamento e diventa un problema serio.</span></p><h5>⚠️ Perdita di controllo sui dati</h5><p><span style="font-weight: 400;">Una volta che i tuoi dati entrano in un sistema AI, non c’è più modo di riprenderli indietro. Sono archiviati su server, magari in un altro continente, e chi garantisce che non vengano usati in futuro per scopi diversi? Pensiamo, ad esempio, a una mamma che carica foto di famiglia su un generatore di immagini AI: quelle immagini potrebbero essere usate per altro, e se domani c’è una violazione o un cambio di policy, potrebbero anche finire in mani sbagliate. </span></p><h5>⚠️ Memorizzazione nascosta e rischio leak</h5><p><span style="font-weight: 400;">Ecco il punto forse più inquietante: i modelli AI apprendono dai dati, e talvolta possono “ricordare” dettagli sensibili, anche se non dovrebbero. Ci sono stati casi in cui i ricercatori hanno dimostrato che un chatbot poteva rigenerare numeri di carte di credito usate durante l’addestramento. E se capita con una carta, perché non con il tuo indirizzo email o il nome del tuo amico Marco?</span></p><h5>⚠️ Repsonsabilità per condivisione di dati riservati</h5><p><span style="font-weight: 400;">Argomento decisamente problematico. Quando condividi con il chatbot un pdf, l'email (magari con dati personali altrui), il codice sorgente, un file excel che desideri rielaborare o riassumere ? Sei sicuro di poterlo fare ? Stai condividendo materiale generico o materiale riservato, magari protetto da diritti autore o segreto industriale? Occorre riflettere e porre attenzione sui materiali che maneggiamo in relazione agli strumenti che utlizziamo. Nel dubbio vale la regola - evitare.</span></p><h5>⚠️ Allucinazioni problematiche o pericolose</h5><p><span style="font-weight: 400;">Le IA non sono infallibili. Possono “inventare” dati – e a volte, quei dati inventati riguardano persone vere. Se il chatbot ti dà la data di nascita sbagliata di un politico, o ti attribuisce un titolo di studio che non hai mai conseguito, si tratta di un problema serio di accuratezza. E correggere questi errori? Spesso impossibile, perché l’IA non sa nemmeno da dove ha preso quell’informazione.</span></p><h5>⚠️ Minori e poca protezione</h5><p><span style="font-weight: 400;">Non dimentichiamo i più giovani: i minorenni usano chatbot senza sempre capire i rischi. Fino al 2023, bastava inserire una data di nascita falsa per usare ChatGPT, anche se avevi 12 anni. Solo dopo interventi dei Garanti (come quello italiano) si è iniziato a introdurre controlli più robusti, ma il problema rimane: i dati dei minori finiscono spesso nel sistema, senza garanzie e consapevolezza.</span></p><h5>⚠️ Data breach e falle di sicurezza</h5><p><span style="font-weight: 400;">Infine, i bug e le violazioni. È già successo (ad esempio a marzo 2024, con ChatGPT) che per errore tecnico alcuni utenti vedessero le chat di altri – inclusi dettagli su abbonamenti e pagamenti. OpenAI all’epoca non avvisò subito tutti i Garanti europei, e questo ha portato a una multa salata. È un promemoria importante: i dati che forniamo a queste piattaforme non sono blindati, e un semplice bug può renderli visibili ad altri.</span></p><hr><h2>Le regole del gioco: GDPR, AI Act e il nodo della privacy nell’IA</h2><p><span style="font-weight: 400;">Cosa rende il mondo dell’intelligenza artificiale così affascinante, ma anche così complicato? Il fatto che corriamo alla velocità della luce – nuovi modelli, nuove app, nuove possibilità – mentre le leggi, beh, fanno un po’ fatica a stare al passo. E non è per cattiveria: regolamentare l’IA è come provare a dare la caccia a un’ombra che cambia forma ogni volta che ti avvicini. Ma vediamo insieme cosa sta succedendo, perché se usi un chatbot, agenti o lavori con questi strumenti, capire le regole è fondamentale.</span></p><h3>Il GDPR: una bussola ancora valida, ma con qualche ammaccatura</h3><p><span style="font-weight: 400;">Partiamo da quello che già conosciamo: il </span><a href="https://eur-lex.europa.eu/legal-content/IT/TXT/?uri=CELEX:32016R0679"><strong>Regolamento Generale sulla Protezione dei Dati (GDPR)</strong></a><span style="font-weight: 400;">. Sì, proprio quello che ci ricorda di leggere le informative privacy (quelle chilometriche che spesso scorriamo velocemente). Il GDPR non è roba vecchia: anche nel 2025 resta </span><strong>la cornice principale</strong><span style="font-weight: 400;"> per la protezione dei dati personali in Europa, e si applica pure ai dati trattati dagli LLM.</span></p><p><span style="font-weight: 400;">Perché è importante? Perché il GDPR stabilisce principi fondamentali come:</span></p><ul><li style="font-weight: 400;" aria-level="1"><strong>Trasparenza</strong><span style="font-weight: 400;">: devi sapere come vengono usati i tuoi dati.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Minimizzazione</strong><span style="font-weight: 400;">: non si possono raccogliere più dati del necessario.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Esattezza</strong><span style="font-weight: 400;">: i dati personali devono essere corretti e aggiornati.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Sicurezza</strong><span style="font-weight: 400;">: le aziende devono proteggere i tuoi dati da violazioni o furti.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">E, soprattutto, </span><strong>diritti dell’interessato</strong><span style="font-weight: 400;">: puoi chiedere di accedere ai tuoi dati, farli cancellare, correggere o opporsi al trattamento.</span><span style="font-weight: 400;"><br><br></span></li></ul><p><span style="font-weight: 400;">Ora, fin qui tutto chiaro, no? Ma il punto è che </span><strong>gli LLM non sono come una rubrica telefonica</strong><span style="font-weight: 400;">. Sono sistemi complessi, che apprendono da quantità immense di dati – inclusi i tuoi post sui social, i tuoi articoli, perfino quel commento lasciato dieci anni fa su un forum dimenticato. E qui casca l’asino: </span><strong>su quale base legale si fonda il loro addestramento?</strong></p><h3>La questione della base giuridica: legittimo interesse o consenso?</h3><p><span style="font-weight: 400;">Prendiamo </span><a href="https://www.agendadigitale.eu/sicurezza/privacy/ai-di-meta-e-davvero-legittimo-interesse-lecito-dubitarne/"><span style="font-weight: 400;">Meta</span></a><span style="font-weight: 400;"> o OpenAI e ChatGPT come esempio. Quando hanno iniziato a raccogliere dati da internet (forum, blog, pagine web) per addestrare i modelli, non hanno chiesto a nessuno. Nessuna email, nessun banner di consenso. Loro hanno detto: “Va bene così, è nel nostro legittimo interesse”.</span></p><p><span style="font-weight: 400;">Il problema? Beh, il GDPR non è così permissivo. Usare dati personali – anche se pubblici – per scopi di addestramento richiede una </span><strong>valutazione rigorosa</strong><span style="font-weight: 400;">, e forse anche il consenso esplicito degli interessati. Il Garante Privacy italiano, infatti, ha sollevato la questione: </span><strong>“Scusate, ma su che base giuridica avete preso e usato questi dati?”</strong></p><p><span style="font-weight: 400;">Risultato: il caso è finito per esempio nelle mani dell’Autorità irlandese (che, per via del meccanismo del “one-stop-shop” in UE, funge da capofila per OpenAI). E qui siamo ancora in attesa di una decisione chiara, ma il messaggio è forte: </span><strong>le regole valgono per tutti, anche per le Big Tech</strong><span style="font-weight: 400;">. Non basta dire “è nel nostro interesse”. Serve dimostrarlo.</span></p><h3>Trasparenza: il nodo delle informative (e la fatica di capirle)</h3><p><span style="font-weight: 400;">Il GDPR impone anche che le aziende spieghino bene come usano i dati. Ma spiegare bene, cosa significa? Facciamo un esempio: per un po’, OpenAI non diceva chiaramente che i prompt degli utenti (cioè le domande fatte a ChatGPT) venivano usati per addestrare il modello. Né specificava che i dati personali inseriti nelle chat potevano finire in mano a revisori umani.</span></p><p><span style="font-weight: 400;">Solo dopo l’intervento del </span><a href="https://www.garanteprivacy.it/home/docweb/-/docweb-display/docweb/10085432"><span style="font-weight: 400;">Garante italiano – con tanto di sanzione da 15 milioni di euro a dicembre 2024 – OpenAI</span></a><span style="font-weight: 400;"> ha dovuto aggiornare la privacy policy, avviare una campagna informativa, e mettere a disposizione opzioni per “uscire” dal training (l’opt-out). Ma resta un problema: </span><strong>anche se disattivi la cronologia, i tuoi dati restano per 30 giorni sui server.</strong><span style="font-weight: 400;"> E, spesso, non è così semplice capire dove finiscano davvero le informazioni.</span></p><p><span style="font-weight: 400;">È un po’ come leggere le clausole scritte in piccolo in un contratto di assicurazione: puoi farlo, ma devi armarti di pazienza – e magari di una lente d’ingrandimento.</span></p><h3>Esattezza e allucinazioni: quando l’IA sbaglia (e non puoi rimediare)</h3><p><span style="font-weight: 400;">Un altro tema caldo è quello dell’</span><strong>accuratezza</strong><span style="font-weight: 400;">. Secondo il GDPR, i dati personali devono essere </span><strong>corretti e aggiornati</strong><span style="font-weight: 400;">. Ma con un LLM, come fai? Se il modello ha imparato che “Mario Rossi è nato nel 1978” e in realtà Mario Rossi è nato nel 1982, come correggi quell’informazione? La risposta breve è: </span><strong>non puoi, almeno non facilmente</strong><span style="font-weight: 400;">.</span></p><h3>Minori e soggetti vulnerabili: tutele deboli, rischi alti</h3><p><span style="font-weight: 400;">Un altro punto dolente riguarda i </span><strong>minori</strong><span style="font-weight: 400;">. Il GDPR dice chiaramente che per usare un servizio online sotto i 16 anni (o 13, in base allo Stato), serve il consenso dei genitori. Eppure, fino a poco tempo fa, chiunque poteva accedere a ChatGPT semplicemente inserendo una data di nascita a caso. Solo dopo il richiamo del Garante, OpenAI ha introdotto controlli più robusti (prima un semplice “age-gate”, poi un sistema più avanzato con verifica di terze parti). Ma siamo ancora in una fase di rodaggio: il sistema funziona davvero? E come si tutelano i dati dei minori una volta dentro il sistema?</span></p><p><span style="font-weight: 400;">Insomma, le falle ci sono. E l’impressione è che le aziende abbiano corso per lanciare i prodotti, senza preoccuparsi troppo delle regole – salvo poi correre ai ripari quando arrivano le multe.</span></p><h3>Il nuovo arrivato: l’AI Act europeo</h3><p><span style="font-weight: 400;">E ora parliamo di lui, l’</span><a href="https://artificialintelligenceact.eu/"><strong>AI Act</strong></a><span style="font-weight: 400;">, che nel 2025 è in dirittura d’arrivo come primo regolamento europeo “omnibus” sull’IA. Il GDPR resta la legge sui dati personali, ma l’AI Act interviene </span><strong>a monte</strong><span style="font-weight: 400;">, fissando regole per lo sviluppo e l’uso delle IA – compresi i modelli generativi come ChatGPT.</span></p><p><span style="font-weight: 400;">Quali sono i punti chiave? Te li riassumo così:</span></p><ul><li style="font-weight: 400;" aria-level="1"><strong>Classificazione per rischio</strong><span style="font-weight: 400;">: l’IA viene divisa in categorie (inaccettabile, alto rischio, basso, minimo). Gli LLM general-purpose non sono “ad alto rischio” di per sé, ma attenzione: se usati in certi contesti (per esempio per la selezione del personale), scattano regole più stringenti.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Trasparenza sui dati di training</strong><span style="font-weight: 400;">: i fornitori dovranno pubblicare un “riassunto” dei dati usati per addestrare il modello. Non tutti i dettagli, ma almeno un’indicazione. Questo per dare un minimo di controllo: sapere se ci sono dentro i tuoi post o quelli di qualcun altro.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Obblighi di sicurezza</strong><span style="font-weight: 400;">: ridurre il rischio di output illegali o discriminatori.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Etichettatura dei contenuti generati</strong><span style="font-weight: 400;">: testi, immagini, video prodotti dall’IA dovranno essere segnalati come tali, per evitare inganni e deepfake.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Divieti specifici</strong><span style="font-weight: 400;">: per esempio, niente più scraping massivo di immagini per creare database di riconoscimento facciale, niente AI per manipolare il comportamento umano in modi dannosi.</span><span style="font-weight: 400;"><br><br></span></li></ul><p><span style="font-weight: 400;">Il messaggio è chiaro: più regole, più responsabilità. E, se non si rispettano, si rischiano multe pesanti: fino a 40 milioni di euro o il 7% del fatturato.</span></p><p><span style="font-weight: 400;">Ma attenzione: l’AI Act non sostituisce il GDPR. Piuttosto, ci lavora insieme: il GDPR tutela i dati a valle, l’AI Act agisce a monte. In pratica, il GDPR dice: “Se tratti dati personali, fallo bene”. L’AI Act aggiunge: “E se sviluppi l’IA, progettala in modo sicuro, trasparente e rispettoso dei diritti delle persone”.</span></p><h2>Casi concreti: quando i rischi diventano realtà</h2><p><span style="font-weight: 400;">A questo punto potresti pensare: “Ok, tutto chiaro, ma questi rischi sono solo ipotesi o sono già successe cose serie?”. Spoiler: sono successe. Eccome, e altre ne succederanno in futuro. Ecco alcuni casi che mostrano come le cose possano andare storte – anche quando si tratta di aziende multimiliardarie.</span></p><h5><strong>🚨 </strong>La sanzione italiana a OpenAI: un campanello d’allarme</h5><p><span style="font-weight: 400;">Dicembre 2024, Italia. OpenAI si becca una multa da 15 milioni di euro per violazioni al GDPR legate a ChatGPT: mancanza di trasparenza, nessun filtro per i minori, inesattezze nei dati, e per finire, una notifica incompleta di un data breach. In pratica, OpenAI aveva creato uno strumento potentissimo, ma si era dimenticata (o aveva trascurato) le regole europee sulla privacy. La multa italiana è stata la prima di questo livello in Europa per un sistema di IA generativa. Un segnale fortissimo, quasi un messaggio in codice: “Ehi, Big Tech, svegliatevi. Le regole valgono anche per voi”.</span></p><p><span style="font-weight: 400;">E guarda caso, dopo questo scossone, altre autorità europee hanno iniziato a muoversi, e il Comitato Europeo per la Protezione dei Dati (EDPB) ha creato una task force su ChatGPT. Segno che, ormai, l’attenzione è alta.</span></p><h5>🚨 Il reclamo NOYB in Austria: l’IA non è infallibile</h5><p><span style="font-weight: 400;">Altro caso interessante:</span><a href="https://noyb.eu/it/chatgpt-provides-false-information-about-people-and-openai-cant-correct-it"><span style="font-weight: 400;"> il reclamo presentato da NOYB</span></a><span style="font-weight: 400;"> (l’associazione fondata da Max Schrems, un nome che chi si occupa di privacy conosce bene) contro OpenAI in Austria. Il problema? Un personaggio pubblico ha chiesto a ChatGPT informazioni su di sé e ha ricevuto dati sbagliati. Ha provato a far correggere l’errore, ma niente da fare: OpenAI ha detto che non può né modificare i dati generati né sapere esattamente da dove provengano.</span></p><h5>🚨 Il bug di ChatGPT (2023): quando le chat private diventano pubbliche</h5><p><span style="font-weight: 400;">Marzo 2023. Un bug tecnico su ChatGPT permette ad alcuni utenti di vedere i titoli delle conversazioni di altri e, in certi casi, dettagli di pagamento. Roba che fa venire i brividi, perché ci ricorda che </span><strong>nulla è davvero privato</strong><span style="font-weight: 400;"> su queste piattaforme. OpenAI ha dovuto correre ai ripari, ma ha commesso un errore grave: non ha notificato subito il data breach a tutte le autorità europee, come richiesto dal GDPR. Questo ha pesato nella sanzione italiana del 2024.</span></p><h5>🚨 Samsung e il divieto interno: un caso significativo</h5><p><span style="font-weight: 400;">Ad aprile 2023, alcuni ingegneri di Samsung – probabilmente in buona fede – hanno caricato pezzi di codice sorgente su ChatGPT per farsi aiutare nel debug. Risultato? Quello stesso codice è finito nei log del chatbot e potenzialmente potrebbe essere stato usato per addestrare modelli futuri. Immagina: un segreto industriale che finisce in un sistema terzo fuori dal tuo controllo. La reazione di Samsung? Un divieto netto: niente più chatbot sulle reti aziendali e sui dispositivi interni. E non sono stati i soli: anche banche come JPMorgan e aziende come Apple hanno preso provvedimenti rigorosi simili. Queste casistiche evidenziano un rischio privacy “indiretto”: non tanto la violazione dei dati personali, quanto la perdita di </span><strong>confidenzialità di dati aziendali</strong><span style="font-weight: 400;"> (che spesso includono anche dati personali di clienti) quando si usano LLM senza cautele o pensiero critico.</span></p><h5>🚨 Zoom e l’aggiornamento delle policy: quando “consenso” diventa un concetto elastico</h5><p><span style="font-weight: 400;">Estate 2023: Zoom aggiorna i suoi termini di servizio e sembra voler usare audio, video e chat delle riunioni per addestrare le proprie IA. Scoppia il caso: utenti e media insorgono, e Zoom fa marcia indietro (almeno in parte). Ma la faccenda resta ambigua: perché se tu, come organizzatore della riunione, accetti certe funzionalità AI (come la trascrizione automatica), di fatto stai anche dando il consenso all’uso dei dati per il training. I partecipanti? Se non vogliono che le loro parole vengano usate, devono semplicemente… non partecipare alla riunione. È un po’ come firmare un contratto senza possibilità di negoziare le clausole: o prendi tutto o niente.</span></p><h2>Le strategie delle Big Tech: tra “data grabbing” e correzioni tardive</h2><p><span style="font-weight: 400;">Ora, fermiamoci un attimo. Perché le Big Tech fanno quello che fanno? È semplice: i modelli generativi – ChatGPT, Claude, Gemini, e compagnia – </span><strong>hanno fame di dati</strong><span style="font-weight: 400;">. E non di due spiccioli: servono trilioni di parole, milioni di immagini, video, registrazioni audio. Più dati hai, più il modello è potente.</span></p><p><span style="font-weight: 400;">Il problema è che per un po’ le regole sono state ignorate. OpenAI, Google, Meta – tutti hanno fatto scraping selvaggio del web: post, articoli, commenti, banche dati opache, perfino dati personali come email o numeri di telefono lasciati su forum dimenticati. E hanno usato questi dati senza chiedere nulla a nessuno. Quando le proteste sono iniziate – autori, artisti, giornalisti, cittadini comuni – le aziende hanno cominciato a fare un passo indietro (o quasi).</span></p><p><span style="font-weight: 400;">OpenAI ha smesso di elencare dettagli sui dataset usati per GPT-4, dopo averlo fatto per GPT-3. Perché? Dicono per ragioni di sicurezza e competizione, ma la verità è che meno si dice, meno problemi si rischia.</span></p><p><span style="font-weight: 400;">Google, invece, ha aggiornato la sua privacy policy per dire chiaramente che usa “dati pubblicamente disponibili” per addestrare l’IA. Ma cosa significa “pubblicamente disponibili”? Anche un post su un blog personale è pubblico, ma non vuol dire che chi lo ha scritto voglia vederlo usato per addestrare un chatbot. È un terreno scivoloso, e il rischio di cause legali è concreto: nel 2023, ad esempio, ci sono state class action contro OpenAI e Meta per l’uso di contenuti protetti da copyright nei training set.</span></p><p><span style="font-weight: 400;">Poi ci sono le correzioni di rotta. OpenAI, dopo il caso italiano, ha introdotto un modulo per richiedere la cancellazione dei dati personali dai risultati di ChatGPT. Un passo avanti, certo. Ma resta una domanda: <strong>chi controlla davvero che questi dati vengano rimossi?</strong> E in quanti sanno che possono fare questa richiesta?</span></p><p><span style="font-weight: 400;">Intanto, le aziende stanno anche sperimentando nuove strategie: partnership con editori (ad esempio OpenAI con Associated Press), licenze con piattaforme come Shutterstock, e modelli “business” a pagamento che promettono più privacy. <strong>È come se la privacy stesse diventando un servizio premium</strong>: se paghi, ti proteggiamo i dati; se usi il servizio gratuito, sappi che i tuoi dati potrebbero servire a migliorare il modello, del resto, come ci ricorda Friedman, in economia “<strong>non esistono pasti gratis</strong>” e quando stai utilizzando qualcosa di gratuito probabilmente il prodotto sei tu.</span></p><h2>E noi? Cosa possiamo fare per proteggerci?</h2><p><span style="font-weight: 400;">A questo punto ci si potrebbe sentire un po’ sopraffatti. Ma non è tutto fuori dal nostro controllo. Ci sono cose concrete che possiamo fare per proteggerci, come utenti e come cittadini.</span></p><ul><li style="font-weight: 400;" aria-level="1"><strong>Non condividere dati sensibili nelle chat AI</strong><span style="font-weight: 400;">: sembra banale, ma spesso ci dimentichiamo che anche un messaggio apparentemente innocuo (“Mio figlio fa la terza media a Milano e ha problemi con la matematica”) contiene dati personali. Meglio evitare.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Controllare le impostazioni di privacy</strong><span style="font-weight: 400;">: molti servizi, come ChatGPT, ora offrono opzioni per non salvare la cronologia o non contribuire al training. In alcuni casi sono nascoste. Usale. E, quando i servizi lo consentono, cancellare la cronologia o disattivare la memoria. </span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Leggere (almeno una volta) le informative privacy</strong><span style="font-weight: 400;">: lo sappiamo, è noioso. Ma almeno una volta proviamo a darci un’occhiata: ci sono spesso dettagli importanti. Atrettanto spesso però cambiano e quindi vanno rilette ogni tanto.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Esercitare i tuoi diritti</strong><span style="font-weight: 400;">: puoi chiedere di accedere ai tuoi dati, di cancellarli, o di opporti al trattamento. È un tuo diritto – usalo.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Se sei un educatore o un genitore, parla di questi temi con i ragazzi</strong><span style="font-weight: 400;">: sono loro i primi a usare queste tecnologie, spesso senza rendersene conto. Un po’ di consapevolezza in più può fare la differenza.</span><span style="font-weight: 400;"><br><br></span></li></ul><h2>In fondo, privacy e IA non sono nemici</h2><p><span style="font-weight: 400;">Lo so, la tentazione è pensare che la privacy e l’IA siano due cose in conflitto: o proteggi i tuoi dati o usi la tecnologia. Ma non deve per forza essere così. Possiamo (e dobbiamo) trovare un equilibrio: costruire IA potenti e utili, ma rispettose della nostra sfera privata, sopratutto etiche e non discriminatorie.</span></p><p><span style="font-weight: 400;">Le leggi ci sono – il GDPR, l’AI Act – ma da sole non bastano. Servono aziende più trasparenti, utenti più consapevoli, e autorità più reattive. <strong>È una sfida collettiva</strong>.</span></p><p><span style="font-weight: 400;">Una cosa è certa: più conosciamo questi strumenti, più possiamo usarli in modo intelligente, senza rinunciare ai nostri diritti. Non dobbiamo smettere di fare domande, di pretendere chiarezza, di chiedere “Scusate, ma i miei dati dove vanno a finire?”. È solo così che l’IA diventerà davvero uno strumento al nostro servizio, e non il contrario.</span></p><h2>Quanti dati sa di te l’AI? Più di quanto immagini (ma meno di quanto pensi)</h2><p>È una domanda che inizia a girare sempre più spesso tra chi usa ChatGPT o altri assistenti intelligenti: <em>“Ma questa AI, quanto sa di me?”</em> Se la usi spesso, potresti avere l’impressione che ti legga nel pensiero. A volte ti anticipa, altre volte ti dà una risposta che suona... troppo su misura. La verità? Un LLM (modello linguistico) non ha una memoria infinita né spia la tua vita – almeno, non nel modo in cui immagini. Ma può <strong>dedurre molte cose da come gli parli</strong>, anche se non gliele hai dette esplicitamente. È un po’ come con un barista che ti vede tutti i giorni: anche se non ti sei mai presentato, sa che prendi il cappuccino con poco zucchero, che arrivi trafelato e che il lunedì sei più silenzioso. Il barista poi ricorda ciò che sente ed è capace di collegarlo ad altre notizie o informazioni. L’AI fa lo stesso, solo che lo fa in silenzio, e in modo statistico.</p><h5>Esperimenti da fare in salotto</h5><p>Vuoi testare quanto un chatbot AI ha imparato su di te? <span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Prova questi piccoli esperimenti. Niente di tecnico, promesso. Prima cosa: chiedi all’AI direttamente </span><em style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">“Cosa sai di me?”</em><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">. Nella maggior parte dei casi, se stai usando un modello chiuso come ChatGPT o Claude, ti risponderà qualcosa tipo </span><em style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">“Non ho informazioni personali su di te”</em><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">. </span><span style="font-weight: 400;">I modelli chiusi come quelli di OpenAI o Anthropic sono </span><strong style="font-family: var(--editor-font-family); font-size: inherit;">programmati per non fornire dati personali</strong><span style="font-weight: 400;"> di individui privati.</span><span style="font-weight: 400;"> </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Ma se sei nella </span><strong style="font-family: var(--editor-font-family); font-size: inherit;">stessa chat</strong><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"> in cui hai già detto “Mi chiamo Lucia e faccio l’infermiera”, potresti scoprire che te lo ripete: ha </span><strong style="font-family: var(--editor-font-family); font-size: inherit;">memoria contestuale</strong><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">, cioè ricorda quello che le hai detto poco fa. Vuoi spingerti oltre? Chiedile: </span><em style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">“Che tipo di personalità pensi io abbia?", "Puoi fare un profilo della mia personalità?”</em><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"> oppure </span><em style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">“Quali argomenti tratto più spesso con te?”</em><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">. </span><span style="font-weight: 400;">Questo invoglia l’AI a sintetizzare i </span><strong style="font-family: var(--editor-font-family); font-size: inherit;">tratti ricorrenti</strong><span style="font-weight: 400;"> che hai mostrato. Potrebbe </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">stupire con un piccolo ritratto basato su ciò che hai scritto. Non è una scheda FBI, ma una </span><strong style="font-family: var(--editor-font-family); font-size: inherit;">fotografia probabilistica</strong><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">. E se usi un servizio con memoria attiva (come la funzione “istruzioni personalizzate” di OpenAI), puoi anche provare: </span><em style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">“Quali dettagli su di me stai usando?”</em><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"> – così verifichi cosa si è tenuta in tasca da altre conversazioni. Si potrebbe tentare anche qualche domanda investigativa inversa </span><strong style="font-family: var(--editor-font-family); font-size: inherit;"><i><span style="font-weight: 400;">“Esaminando come ti ho posto le domande finora, noti qualche errore o abitudine sbagliata che ho quando chiedo qualcosa?”, "Puoi dirmi quali sono i tipi di aiuto o gli argomenti che ti ho chiesto più spesso finora?”</span></i><span style="font-weight: 400;">. Questo costringe l’AI a ripensare alle tue interazioni e magari citare esempi di domande che hai fatto. </span></strong><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Potremmo indagare il cosidetto "blind spot" </span><strong style="font-family: var(--editor-font-family); font-size: inherit;"><span style="font-weight: 400;">chiedendo <i>“Basandoti sulle nostre conversazioni, c’è qualcosa di significativo che secondo te mi sfugge o tendo a ignorare?”. </i></span><span style="font-weight: 400;">È come chiedere un parere esterno sulle tue abitudini. </span></strong><span style="font-weight: 400;">Questo non è tanto un dato “raccolto”, quanto una </span><span style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">inferenza</span><strong style="font-family: var(--editor-font-family); font-size: inherit;"><span style="font-weight: 400;">: l’AI rielabora i dati delle tue interazioni per darti un feedback su di te e questo ti fa capire come il modello analizza e profila in qualche modo il tuo comportamento.</span></strong></p><p>Teniamo bene a mente che sono sempre tutte <strong>risposte di tipo probabilistico </strong>(senza intelligenza) e quindi vanno prese con leggerezza e nella consapevolezza che <strong>possono essere errate.</strong></p><h5>Un dossier invisibile? No, ma meglio restare svegli</h5><p>C’è però un’altra faccia della medaglia. Anche se il modello stesso non ti può elencare i dati raccolti (non è progettato per questo), <strong>la piattaforma che lo gestisce sì</strong>. Dietro le quinte, i server registrano la cronologia delle chat, l’orario, il tipo di dispositivo usato, la posizione approssimativa. Principalmente per motivi tecnici e legali. Il problema è che tu, utente comune, <strong>non puoi vedere tutto questo in chiaro</strong>: non c’è un pulsante “<em>Scarica ciò che sai di me</em>” o “<em>Cancella tutto e dimenticati di me</em>” accessibile direttamente dall’AI. Vuoi sapere davvero quali dati hanno memorizzato? Devi fare una richiesta formale al provider – ad esempio tramite i moduli privacy di OpenAI – oppure disattivare le funzioni di cronologia e memoria, e purtroppo una vera cancellazione non sempre è possibile, ancor più oggi e in futuro dove i sistemi sono e stanno diventando sempre più Agenti, inevitabilmente condividono sempre più dati con sistemi terzi (anche a noi ignoti). Insomma, l’<strong>AI non ha una sfera di cristallo</strong>, ma se la alimenti a lungo e le lasci indizi... <strong>impara a riconoscerti</strong>. Con misura e consapevolezza, può essere uno specchio utile e potrebbe diventare anche uno strumento introspettivo interessante. Ma non dimenticare: ogni volta che parli con un sistema AI, <strong>non sei mai del tutto solo</strong>.</p><p>Un consiglio guida potrebbe essere “<strong>mai condividere cose che normalmente non condivideresti in pubblico</strong>”.</p><hr><p class="align-center">Questo post è parte della rubrica <strong><a href="https://pianetararo.org/traiettorie/">TrAIettorie</a></strong> di cui potete trovare l'indice completo <a href="https://pianetararo.org/tags/traiettorie/">qui</a>.</p></div><footer class="content__footer"><div class="entry-wrapper"><div class="content__actions"><ul class="content__tag"><li><a href="https://pianetararo.org/tags/traiettorie/">TRAIETTORIE</a></li></ul><div class="content__share"><button class="btn--icon content__share-button js-content__share-button"><svg width="20" height="20" aria-hidden="true"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#share"></use></svg> <span>Share It</span></button><div class="content__share-popup js-content__share-popup"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fpianetararo.org%2Fprivacy-e-intelligenza-artificiale-cosa-dobbiamo-sapere-nel-2025%2F" class="js-share facebook" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#facebook"/></svg> <span>Facebook</span> </a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fpianetararo.org%2Fprivacy-e-intelligenza-artificiale-cosa-dobbiamo-sapere-nel-2025%2F" class="js-share linkedin" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#linkedin"/></svg> <span>LinkedIn</span> </a><a href="https://api.whatsapp.com/send?text=Privacy%20e%20intelligenza%20artificiale%3A%20cosa%20dobbiamo%20sapere%20nel%202025 https%3A%2F%2Fpianetararo.org%2Fprivacy-e-intelligenza-artificiale-cosa-dobbiamo-sapere-nel-2025%2F" class="js-share whatsapp" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#whatsapp"/></svg> <span>WhatsApp</span></a></div></div></div></div></footer></article></main><footer class="footer"><div class="wrapper"><div class="footer__copyright"><div class="block-footer-fl" style="width: 100%; height: 100%; , max-width: 100%;"><p class="align-center" style="font-size: x-small;"><span style="color: #44684b;">Pianetararo associazione culturale</span><br><span style="color: #44684b;">CF: 04015870365</span><br><span style="color: #44684b;">info@pianetararo.org</span><br><span style="color: #44684b;">Pianetararo è un associazione senza scopo di lucro e partecipa al programma "Google for Non profits".</span></p><p class="align-center" style="font-size: x-small;"><span style="color: #44684b;"><a href="https://pianetararo.org/privacy-and-cookie/" title="Privacy &amp; Cookie policy" style="color: #44684b;">Privacy &amp; Cookie policy</a></span></p></div></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg width="20" height="20"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#toparrow"/></svg></button></div></footer><script defer="defer" src="https://pianetararo.org/assets/js/scripts.min.js?v=700105c316933a8202041b6415abb233"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script><div class="pcb" data-behaviour="badge" data-behaviour-link="#cookie-settings" data-revision="1" data-config-ttl="90" data-debug-mode="false"><div role="dialog" aria-modal="true" aria-hidden="true" aria-labelledby="pcb-title" aria-describedby="pcb-txt" class="pcb__banner"><div class="pcb__inner"><div id="pcb-title" role="heading" aria-level="2" class="pcb__title">This website uses cookies</div><div id="pcb-txt" class="pcb__txt">Select which cookies to opt-in to via the checkboxes below; our website uses cookies to examine site traffic and user activity while on our site, for marketing, and to provide social media functionality. <a href="https://pianetararo.org/privacy-and-cookie/">More details...</a></div><div class="pcb__buttons"><button type="button" class="pcb__btn pcb__btn--solid pcb__btn--accept">Accept all</button></div></div></div><button class="pcb__badge" aria-label="Cookie Policy" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" width="40" height="40" viewBox="0 0 23 23" fill="currentColor"><path d="M21.41 12.71c-.08-.01-.15 0-.22 0h-.03c-.03 0-.05 0-.08.01-.07 0-.13.01-.19.04-.52.21-1.44.19-2.02-.22-.44-.31-.65-.83-.62-1.53a.758.758 0 0 0-.27-.61.73.73 0 0 0-.65-.14c-1.98.51-3.49.23-4.26-.78-.82-1.08-.73-2.89.24-4.49.14-.23.14-.52 0-.75a.756.756 0 0 0-.67-.36c-.64.03-1.11-.1-1.31-.35-.19-.26-.13-.71-.01-1.29.04-.18.06-.38.03-.59-.05-.4-.4-.7-.81-.66C5.1 1.54 1 6.04 1 11.48 1 17.28 5.75 22 11.6 22c5.02 0 9.39-3.54 10.39-8.42.08-.4-.18-.78-.58-.87Zm-9.81 7.82c-5.03 0-9.12-4.06-9.12-9.06 0-4.34 3.05-8 7.25-8.86-.08.7.05 1.33.42 1.81.24.32.66.67 1.38.84-.76 1.86-.65 3.78.36 5.11.61.81 2.03 2 4.95 1.51.18.96.71 1.54 1.18 1.87.62.43 1.38.62 2.1.62.05 0 .09 0 .13-.01-1.23 3.64-4.7 6.18-8.64 6.18ZM13 17c0 .55-.45 1-1 1s-1-.45-1-1 .45-1 1-1 1 .45 1 1Zm5.29-12.3a.99.99 0 0 1-.29-.71c0-.55.45-.99 1-.99a1 1 0 0 1 .71.3c.19.19.29.44.29.71 0 .55-.45.99-1 .99a1 1 0 0 1-.71-.3ZM9 13.5c0 .83-.67 1.5-1.5 1.5S6 14.33 6 13.5 6.67 12 7.5 12s1.5.67 1.5 1.5Zm3.25.81a.744.744 0 0 1-.06-1.05c.28-.32.75-.34 1.05-.06.31.28.33.75.05 1.06-.15.16-.35.25-.56.25-.18 0-.36-.06-.5-.19ZM8.68 7.26c.41.37.44 1 .07 1.41-.2.22-.47.33-.75.33a.96.96 0 0 1-.67-.26c-.41-.37-.44-1-.07-1.41.37-.42 1-.45 1.41-.08Zm11.48 1.88c.18-.19.52-.19.7 0 .05.04.09.1.11.16.03.06.04.12.04.19 0 .13-.05.26-.15.35-.09.1-.22.15-.35.15s-.26-.05-.35-.15a.355.355 0 0 1-.11-.16.433.433 0 0 1-.04-.19c0-.13.05-.26.15-.35Zm-4.93-1.86a.75.75 0 1 1 1.059-1.06.75.75 0 0 1-1.059 1.06Z"/></svg></button></div><script>(function(win) {
    if (!document.querySelector('.pcb')) {
        return;
    }

    var cbConfig = {
        behaviour: document.querySelector('.pcb').getAttribute('data-behaviour'),
        behaviourLink: document.querySelector('.pcb').getAttribute('data-behaviour-link'),
        revision: document.querySelector('.pcb').getAttribute('data-revision'),
        configTTL: parseInt(document.querySelector('.pcb').getAttribute('data-config-ttl'), 10),
        debugMode: document.querySelector('.pcb').getAttribute('data-debug-mode') === 'true',
        initialState: null,
        initialLsState: null,
        previouslyAccepted: []
    };

    var cbUI = {
        wrapper: document.querySelector('.pcb'),
        banner: {
            element: null,
            btnAccept: null,
            btnReject: null,
            btnConfigure: null
        },
        popup: {
            element: null,
            btnClose: null,
            btnSave: null,
            btnAccept: null,
            btnReject: null,
            checkboxes: null,
        },
        overlay: null,
        badge: null,
        blockedScripts: document.querySelectorAll('script[type^="gdpr-blocker/"]'),
        triggerLinks: cbConfig.behaviourLink ? document.querySelectorAll('a[href*="' + cbConfig.behaviourLink + '"]') : null
    };

    function initUI () {
        // setup banner elements
        cbUI.banner.element = cbUI.wrapper.querySelector('.pcb__banner');
        cbUI.banner.btnAccept = cbUI.banner.element.querySelector('.pcb__btn--accept');
        cbUI.banner.btnReject = cbUI.banner.element.querySelector('.pcb__btn--reject');
        cbUI.banner.btnConfigure = cbUI.banner.element.querySelector('.pcb__btn--configure');

        // setup popup elements
        if (cbUI.wrapper.querySelector('.pcb__popup')) {
            cbUI.popup.element = cbUI.wrapper.querySelector('.pcb__popup');
            cbUI.popup.btnClose = cbUI.wrapper.querySelector('.pcb__popup__close');
            cbUI.popup.btnSave = cbUI.popup.element.querySelector('.pcb__btn--save');
            cbUI.popup.btnAccept = cbUI.popup.element.querySelector('.pcb__btn--accept');
            cbUI.popup.btnReject = cbUI.popup.element.querySelector('.pcb__btn--reject');
            cbUI.popup.checkboxes = cbUI.popup.element.querySelector('input[type="checkbox"]');
            // setup overlay
            cbUI.overlay = cbUI.wrapper.querySelector('.pcb__overlay');
        }

        cbUI.badge = cbUI.wrapper.querySelector('.pcb__badge');

        if (cbConfig.behaviour.indexOf('link') > -1) {
            for (var i = 0; i < cbUI.triggerLinks.length; i++) {
                cbUI.triggerLinks[i].addEventListener('click', function(e) {
                    e.preventDefault();
                    showBannerOrPopup();
                });
            }
        }
    }

    function initState () {
        var lsKeyName = getConfigName();
        var currentConfig = localStorage.getItem(lsKeyName);
        var configIsFresh = checkIfConfigIsFresh();

        if (!configIsFresh || currentConfig === null) {
            if (cbConfig.debugMode) {
                console.log('🍪 Config not found, or configuration expired');
            }

            if (window.publiiCBGCM) {
                gtag('consent', 'default', {
                    'ad_storage': window.publiiCBGCM.defaultState.ad_storage ? 'granted' : 'denied',
                    'ad_personalization': window.publiiCBGCM.defaultState.ad_personalization ? 'granted' : 'denied',
                    'ad_user_data': window.publiiCBGCM.defaultState.ad_user_data ? 'granted' : 'denied',
                    'analytics_storage': window.publiiCBGCM.defaultState.analytics_storage ? 'granted' : 'denied',
                    'personalization_storage': window.publiiCBGCM.defaultState.personalization_storage ? 'granted' : 'denied',
                    'functionality_storage': window.publiiCBGCM.defaultState.functionality_storage ? 'granted' : 'denied',
                    'security_storage': window.publiiCBGCM.defaultState.security_storage ? 'granted' : 'denied'
                });  
                
                if (cbConfig.debugMode) {
                    console.log('🍪 GCMv2 DEFAULT STATE: ' + JSON.stringify({
                        'ad_storage': window.publiiCBGCM.defaultState.ad_storage ? 'granted' : 'denied',
                        'ad_personalization': window.publiiCBGCM.defaultState.ad_personalization ? 'granted' : 'denied',
                        'ad_user_data': window.publiiCBGCM.defaultState.ad_user_data ? 'granted' : 'denied',
                        'analytics_storage': window.publiiCBGCM.defaultState.analytics_storage ? 'granted' : 'denied',
                        'personalization_storage': window.publiiCBGCM.defaultState.personalization_storage ? 'granted' : 'denied',
                        'functionality_storage': window.publiiCBGCM.defaultState.functionality_storage ? 'granted' : 'denied',
                        'security_storage': window.publiiCBGCM.defaultState.security_storage ? 'granted' : 'denied'
                    }));
                }
            }

            showBanner();
        } else if (typeof currentConfig === 'string') {
            if (cbConfig.debugMode) {
                console.log('🍪 Config founded');
            }

            cbConfig.initialLsState = currentConfig.split(',');

            if (window.publiiCBGCM) {
                gtag('consent', 'default', {
                    'ad_storage': getDefaultConsentState(currentConfig, 'ad_storage'),
                    'ad_personalization': getDefaultConsentState(currentConfig, 'ad_personalization'),
                    'ad_user_data': getDefaultConsentState(currentConfig, 'ad_user_data'),
                    'analytics_storage': getDefaultConsentState(currentConfig, 'analytics_storage'),
                    'personalization_storage': getDefaultConsentState(currentConfig, 'personalization_storage'),
                    'functionality_storage': getDefaultConsentState(currentConfig, 'functionality_storage'),
                    'security_storage': getDefaultConsentState(currentConfig, 'security_storage')
                });
                
                if (cbConfig.debugMode) {
                    console.log('🍪 GCMv2 DEFAULT STATE: ' + JSON.stringify({
                        'ad_storage': getDefaultConsentState(currentConfig, 'ad_storage'),
                        'ad_personalization': getDefaultConsentState(currentConfig, 'ad_personalization'),
                        'ad_user_data': getDefaultConsentState(currentConfig, 'ad_user_data'),
                        'analytics_storage': getDefaultConsentState(currentConfig, 'analytics_storage'),
                        'personalization_storage': getDefaultConsentState(currentConfig, 'personalization_storage'),
                        'functionality_storage': getDefaultConsentState(currentConfig, 'functionality_storage'),
                        'security_storage': getDefaultConsentState(currentConfig, 'security_storage')
                    }));
                }
            }

            showBadge();

            if (cbUI.popup.element) {
                var allowedGroups = currentConfig.split(',');
                var checkedCheckboxes = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');

                for (var j = 0; j < checkedCheckboxes.length; j++) {
                    var name = checkedCheckboxes[j].getAttribute('data-group-name');

                    if (name && name !== '-' && allowedGroups.indexOf(name) === -1) {
                        checkedCheckboxes[j].checked = false;
                    }
                }

                for (var i = 0; i < allowedGroups.length; i++) {
                    var checkbox = cbUI.popup.element.querySelector('input[type="checkbox"][data-group-name="' + allowedGroups[i] + '"]');

                    if (checkbox) {
                        checkbox.checked = true;
                    }

                    allowCookieGroup(allowedGroups[i]);
                }
            }
        }

        setTimeout(function () {
            cbConfig.initialState = getInitialStateOfConsents();
        }, 0);
    }

    function checkIfConfigIsFresh () {
        var lastConfigSave = localStorage.getItem('publii-gdpr-cookies-config-save-date');

        if (lastConfigSave === null) {
            return false;
        }

        lastConfigSave = parseInt(lastConfigSave, 10);

        if (lastConfigSave === 0) {
            return true;
        }

        if (+new Date() - lastConfigSave < cbConfig.configTTL * 24 * 60 * 60 * 1000) {
            return true;
        }

        return false;
    }

    function getDefaultConsentState (currentConfig, consentGroup) {
        let configGroups = currentConfig.split(',');

        for (let i = 0; i < configGroups.length; i++) {
            let groupName = configGroups[i];
            let group = window.publiiCBGCM.groups.find(group => group.cookieGroup === groupName);

            if (group && group[consentGroup]) {
                return 'granted';
            }
        }  
        
        if (window.publiiCBGCM.defaultState[consentGroup]) {
            return 'granted'; 
        }
        
        return 'denied';
    }

    function initBannerEvents () {
        cbUI.banner.btnAccept.addEventListener('click', function (e) {
            e.preventDefault();
            acceptAllCookies('banner');
            showBadge();
        }, false);

        if (cbUI.banner.btnReject) {
            cbUI.banner.btnReject.addEventListener('click', function (e) {
                e.preventDefault();
                rejectAllCookies();
                showBadge();
            }, false);
        }

        if (cbUI.banner.btnConfigure) {
            cbUI.banner.btnConfigure.addEventListener('click', function (e) {
                e.preventDefault();
                hideBanner();
                showAdvancedPopup();
                showBadge();
            }, false);
        }
    }

    function initPopupEvents () {
        if (!cbUI.popup.element) {
            return;
        }

        cbUI.overlay.addEventListener('click', function (e) {
            hideAdvancedPopup();
        }, false);

        cbUI.popup.element.addEventListener('click', function (e) {
            e.stopPropagation();
        }, false);

        cbUI.popup.btnAccept.addEventListener('click', function (e) {
            e.preventDefault();
            acceptAllCookies('popup');
        }, false);

        cbUI.popup.btnReject.addEventListener('click', function (e) {
            e.preventDefault();
            rejectAllCookies();
        }, false);

        cbUI.popup.btnSave.addEventListener('click', function (e) {
            e.preventDefault();
            saveConfiguration();
        }, false);

        cbUI.popup.btnClose.addEventListener('click', function (e) {
            e.preventDefault();
            hideAdvancedPopup();
        }, false);
    }

    function initBadgeEvents () {
        if (!cbUI.badge) {
            return;
        }

        cbUI.badge.addEventListener('click', function (e) {
            showBannerOrPopup();
        }, false);
    }

    initUI();
    initState();
    initBannerEvents();
    initPopupEvents();
    initBadgeEvents();

    /**
     * API
     */
    function addScript (src, inline) {
        var newScript = document.createElement('script');

        if (src) {
            newScript.setAttribute('src', src);
        }

        if (inline) {
            newScript.text = inline;
        }

        document.body.appendChild(newScript);
    }

    function allowCookieGroup (allowedGroup) {
        var scripts = document.querySelectorAll('script[type="gdpr-blocker/' + allowedGroup + '"]');
        cbConfig.previouslyAccepted.push(allowedGroup);
    
        for (var j = 0; j < scripts.length; j++) {
            addScript(scripts[j].src, scripts[j].text);
        }

        var groupEvent = new Event('publii-cookie-banner-unblock-' + allowedGroup);
        document.body.dispatchEvent(groupEvent);
        unlockEmbeds(allowedGroup);

        if (cbConfig.debugMode) {
            console.log('🍪 Allowed group: ' + allowedGroup);
        }

        if (window.publiiCBGCM && cbConfig.initialLsState.indexOf(allowedGroup) === -1) {
            let consentResult = {};
            let group = window.publiiCBGCM.groups.find(group => group.cookieGroup === allowedGroup);

            if (group) {
                let foundSomeConsents = false;

                Object.keys(group).forEach(key => {
                    if (key !== 'cookieGroup' && group[key] === true) {
                        consentResult[key] = 'granted';
                        foundSomeConsents = true;
                    }
                });

                if (foundSomeConsents) {
                    gtag('consent', 'update', consentResult);   

                    if (cbConfig.debugMode) {
                        console.log('🍪 GCMv2 UPDATE: ' + JSON.stringify(consentResult));
                    }
                }
            }
        }
    }

    function showBannerOrPopup () {
        if (cbUI.popup.element) {
            showAdvancedPopup();
        } else {
            showBanner();
        }
    }

    function showAdvancedPopup () {
        cbUI.popup.element.classList.add('is-visible');
        cbUI.overlay.classList.add('is-visible');
        cbUI.popup.element.setAttribute('aria-hidden', 'false');
        cbUI.overlay.setAttribute('aria-hidden', 'false');
    }

    function hideAdvancedPopup () {
        cbUI.popup.element.classList.remove('is-visible');
        cbUI.overlay.classList.remove('is-visible');
        cbUI.popup.element.setAttribute('aria-hidden', 'true');
        cbUI.overlay.setAttribute('aria-hidden', 'true');
    }

    function showBanner () {
        cbUI.banner.element.classList.add('is-visible');
        cbUI.banner.element.setAttribute('aria-hidden', 'false');
    }

    function hideBanner () {
        cbUI.banner.element.classList.remove('is-visible');
        cbUI.banner.element.setAttribute('aria-hidden', 'true');
    }

    function showBadge () {
        if (!cbUI.badge) {
            return;
        }

        cbUI.badge.classList.add('is-visible');
        cbUI.badge.setAttribute('aria-hidden', 'false');
    }

    function getConfigName () {
        var lsKeyName = 'publii-gdpr-allowed-cookies';

        if (cbConfig.revision) {
            lsKeyName = lsKeyName + '-v' + parseInt(cbConfig.revision, 10);
        }

        return lsKeyName;
    }

    function storeConfiguration (allowedGroups) {
        var lsKeyName = getConfigName();
        var dataToStore = allowedGroups.join(',');
        localStorage.setItem(lsKeyName, dataToStore);

        if (cbConfig.configTTL === 0) {
            localStorage.setItem('publii-gdpr-cookies-config-save-date', 0);

            if (cbConfig.debugMode) {
                console.log('🍪 Store never expiring configuration');
            }
        } else {
            localStorage.setItem('publii-gdpr-cookies-config-save-date', +new Date());
        }
    }

    function getInitialStateOfConsents () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        if (cbConfig.debugMode) {
            console.log('🍪 Initial state: ' + groups.join(', '));
        }

        return groups;
    }

    function getCurrentStateOfConsents () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        if (cbConfig.debugMode) {
            console.log('🍪 State to save: ' + groups.join(', '));
        }

        return groups;
    }

    function getAllGroups () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        return groups;
    }

    function acceptAllCookies (source) {
        var groupsToAccept = getAllGroups();
        storeConfiguration(groupsToAccept);

        for (var i = 0; i < groupsToAccept.length; i++) {
            var group = groupsToAccept[i];

            if (cbConfig.initialState.indexOf(group) > -1 || cbConfig.previouslyAccepted.indexOf(group) > -1) {
                if (cbConfig.debugMode) {
                    console.log('🍪 Skip previously activated group: ' + group);
                }

                continue;
            }

            allowCookieGroup(group);
        }

        if (cbUI.popup.element) {
            var checkboxesToCheck = cbUI.popup.element.querySelectorAll('input[type="checkbox"]');

            for (var j = 0; j < checkboxesToCheck.length; j++) {
                checkboxesToCheck[j].checked = true;
            }
        }

        if (cbConfig.debugMode) {
            console.log('🍪 Accept all cookies: ', groupsToAccept.join(', '));
        }

        if (source === 'popup') {
            hideAdvancedPopup();
        } else if (source === 'banner') {
            hideBanner();
        }
    }

    function rejectAllCookies () {
        if (cbConfig.debugMode) {
            console.log('🍪 Reject all cookies');
        }

        storeConfiguration([]);
        setTimeout(function () {
            window.location.reload();
        }, 100);
    }

    function saveConfiguration () {
        var groupsToAccept = getCurrentStateOfConsents();
        storeConfiguration(groupsToAccept);

        if (cbConfig.debugMode) {
            console.log('🍪 Save new config: ', groupsToAccept.join(', '));
        }

        if (reloadIsNeeded(groupsToAccept)) {
            setTimeout(function () {
                window.location.reload();
            }, 100);
            return;
        }

        for (var i = 0; i < groupsToAccept.length; i++) {
            var group = groupsToAccept[i];

            if (cbConfig.initialState.indexOf(group) > -1 || cbConfig.previouslyAccepted.indexOf(group) > -1) {
                if (cbConfig.debugMode) {
                    console.log('🍪 Skip previously activated group: ' + group);
                }

                continue;
            }

            allowCookieGroup(group);
        }

        hideAdvancedPopup();
    }

    function reloadIsNeeded (groupsToAccept) {
        // check if user rejected consent for initial groups
        var initialGroups = cbConfig.initialState;
        var previouslyAcceptedGroups = cbConfig.previouslyAccepted;
        var groupsToCheck = initialGroups.concat(previouslyAcceptedGroups);

        for (var i = 0; i < groupsToCheck.length; i++) {
            var groupToCheck = groupsToCheck[i];

            if (groupToCheck !== '' && groupsToAccept.indexOf(groupToCheck) === -1) {
                if (cbConfig.debugMode) {
                    console.log('🍪 Reload is needed due lack of: ', groupToCheck);
                }

                return true;
            }
        }

        return false;
    }

    function unlockEmbeds (cookieGroup) {
        var iframesToUnlock = document.querySelectorAll('.pec-wrapper[data-consent-group-id="' + cookieGroup + '"]');

        for (var i = 0; i < iframesToUnlock.length; i++) {
            var iframeWrapper = iframesToUnlock[i];
            iframeWrapper.querySelector('.pec-overlay').classList.remove('is-active');
            iframeWrapper.querySelector('.pec-overlay').setAttribute('aria-hidden', 'true');
            var iframe = iframeWrapper.querySelector('iframe');
            iframe.setAttribute('src', iframe.getAttribute('data-consent-src'));
        }
    }

    win.publiiEmbedConsentGiven = function (cookieGroup) {
        // it will unlock embeds
        allowCookieGroup(cookieGroup);

        var checkbox = cbUI.popup.element.querySelector('input[type="checkbox"][data-group-name="' + cookieGroup + '"]');

        if (checkbox) {
            checkbox.checked = true;
        }

        var groupsToAccept = getCurrentStateOfConsents();
        storeConfiguration(groupsToAccept);

        if (cbConfig.debugMode) {
            console.log('🍪 Save new config: ', groupsToAccept.join(', '));
        }
    }
})(window);</script></body></html>