<!DOCTYPE html><html lang="it"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Armi di distruzione matematica (Cathy O’Neil) - pianetararo</title><meta name="description" content="Esamina il libro di Cathy O’Neil su algoritmi ingiusti: esempi reali, effetti sociali e riflessioni su bias, trasparenza e democrazia."><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://pianetararo.org/armi-di-distruzione-matematica-cathy-oneil/"><link rel="alternate" type="application/atom+xml" href="https://pianetararo.org/feed.xml"><link rel="alternate" type="application/json" href="https://pianetararo.org/feed.json"><meta property="og:title" content="Armi di distruzione matematica (Cathy O’Neil)"><meta property="og:image" content="https://pianetararo.org/media/posts/23/Gemini_Generated_Image_mh5uqymh5uqymh5u-2.jfif"><meta property="og:image:width" content="2048"><meta property="og:image:height" content="2048"><meta property="og:site_name" content="pianetararo"><meta property="og:description" content="Esamina il libro di Cathy O’Neil su algoritmi ingiusti: esempi reali, effetti sociali e riflessioni su bias, trasparenza e democrazia."><meta property="og:url" content="https://pianetararo.org//armi-di-distruzione-matematica-cathy-oneil/"><meta property="og:type" content="article"><link rel="stylesheet" href="https://pianetararo.org/assets/css/style.css?v=812e0178178abea4ea9399c6007c2ff4"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://pianetararo.org/armi-di-distruzione-matematica-cathy-oneil/"},"headline":"Armi di distruzione matematica (Cathy O’Neil)","datePublished":"2025-05-01T21:27+02:00","dateModified":"2025-06-13T16:17+02:00","image":{"@type":"ImageObject","url":"https://pianetararo.org/media/posts/23/Gemini_Generated_Image_mh5uqymh5uqymh5u-2.jfif","height":2048,"width":2048},"description":"Esamina il libro di Cathy O’Neil su algoritmi ingiusti: esempi reali, effetti sociali e riflessioni su bias, trasparenza e democrazia.","author":{"@type":"Person","name":"Pianetararo Associazione Culturale","url":"https://pianetararo.org/authors/pianetararo-associazione-culturale/"},"publisher":{"@type":"Organization","name":"Pianetararo Associazione Culturale","logo":{"@type":"ImageObject","url":"https://pianetararo.org/media/website/PIANETARARO-1-1-1.svg","height":375,"width":375}}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><header class="top js-header"><a class="logo" href="https://pianetararo.org/"><img src="https://pianetararo.org/media/website/PIANETARARO-1-1-1.svg" alt="pianetararo" width="375" height="375"></a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://pianetararo.org/test-2/" title="pianetararo" target="_self">Chi siamo</a></li><li><a href="https://pianetararo.org/traiettorie/" target="_self">TrAIettorie</a></li><li class="has-submenu"><a href="https://pianetararo.org/frammenti25intro/" title="frammenti25" target="_self" aria-haspopup="true">FRAMMENTI#25</a><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/" target="_self">FRAMMENTI DIGITAL - Come funziona ?</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/1-il-pozzo-di-san-patrizio/" target="_self">#1 - Gennaio</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/2-la-rocca-di-calascio/" target="_self">#2 - Febbraio</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/3-il-palio-delle-rane/" target="_self">#3 - Marzo</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/4-il-carnevale-di-mamoiada/" target="_self">#4 - Aprile</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/5-il-castello-di-montebello-e-la-dama-bianca/" target="_self">#5 - Maggio</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/6/" target="_self">#6 - Giugno</a></li></ul></li><li><a href="https://pianetararo.org/stradora/" title="STRADORA" target="_self">STRADORA</a></li><li><a href="https://pianetararo.org/pensieri/" title="PENSIERI" target="_self">Pensieri</a></li><li><a href="https://pianetararo.org/tags/" target="_self">Tags</a></li></ul></nav></header><main class="post"><article class="content"><div class="hero"><header class="hero__content"><div class="wrapper"><h1>Armi di distruzione matematica (Cathy O’Neil)</h1><div class="feed__meta content__meta"><time datetime="2025-05-01T21:27" class="feed__date">1 mag 2025</time></div></div></header><figure class="hero__image"><div class="hero__image-wrapper"><img src="https://pianetararo.org/media/posts/23/Gemini_Generated_Image_mh5uqymh5uqymh5u-2.jfif" loading="eager" height="2048" width="2048" alt=""></div></figure></div><div class="entry-wrapper content__entry"><h4 class="align-center"><strong>Quando l’algoritmo diventa ingiusto senza che ce ne accorgiamo</strong></h4><p class="align-center"><em>(Recensione e riflessioni ispirate al libro di Cathy O’Neil)</em></p><hr><p>C’è un’illusione di fondo che ancora ci accompagna quando pensiamo ai numeri, alle formule, agli algoritmi: quella di una matematica “<em>neutrale</em>”, distaccata, quasi al di sopra dei pregiudizi umani. <strong>Cathy O’Neil</strong>, con il suo libro “<strong><a href="https://search.worldcat.org/it/title/1015979069">Armi di distruzione matematica</a></strong>”, demolisce questa illusione pezzo dopo pezzo, e lo fa con una chiarezza e una passione che davvero costringono a rimettere in discussione tutto ciò che pensavamo di sapere sul potere dei dati.</p><p>Per chi non l’avesse ancora letto – consigliamo vivamente di farlo, magari anche solo per farsi due domande scomode davanti alla prossima richiesta di “accetta i cookie” – O’Neil parte proprio dal suo percorso personale. Matematica di formazione, finita tra hedge fund e startup tech, lei per prima aveva creduto che più numeri significasse più giustizia. Invece, a Wall Street, ha visto i modelli matematici gonfiare la bolla dei subprime, amplificare rischi e ingiustizie, e, paradossalmente, fornire una giustificazione “scientifica” a decisioni che poi, di scientifico, avevano ben poco.</p><p>La cosa che colpisce è come i modelli, una volta usciti dai laboratori e dalle simulazioni accademiche, si siano infilati in ogni anfratto della società. Oggi li troviamo ovunque: dalla scuola ai tribunali, dalle banche alle pubblicità che ci inseguono online. E non solo: decidono chi riceverà un prestito, chi sarà chiamato per un colloquio, chi verrà licenziato, chi dovrà pagare di più per l’assicurazione auto, chi sarà sorvegliato da una pattuglia di polizia. Sembra il set di un film distopico, e invece è la routine di ogni giorno.</p><p>Leggendo O’Neil sorge una domanda semplice e spiazzante: <strong>quando un modello matematico diventa pericoloso?</strong> Non basta dire “quando sbaglia”, perché anche un modello ben fatto può sbagliare di tanto in tanto. Il problema nasce, piuttosto, quando un algoritmo si fa opaco, si applica su vasta scala e produce danni sistemici, colpendo soprattutto chi ha meno mezzi per difendersi. Un po’ come una burocrazia impazzita: fredda, senza volto, incapace di ascoltare. Lì, sì, che diventa un’arma – un’arma di distruzione matematica.</p><p>Un esempio raccontato – che resta in testa come una piccola ingiustizia che nessuno ha voglia di raccontare – è la storia di Sarah Wysocki, insegnante a Washington. Amata da studenti e genitori, si vede licenziata all’improvviso perché un algoritmo, il famoso IMPACT, l’aveva bollata come “tra i peggiori docenti”. Poco importa se il modello, a monte, era fallato (bastava che l’anno prima fossero stati truccati i risultati dei test degli studenti per falsare tutto). Poco importa il contesto reale, la storia personale, la voce di chi la conosceva davvero. Il numero parla, il destino si compie. E nessuno che possa contestare, spiegare, nemmeno appellarsi. Così nasce il circolo vizioso: più il modello punisce, più la gente impara a “giocare con le regole” – truccando dati, aggirando ostacoli – e meno il sistema assomiglia a ciò che dovrebbe valutare.</p><p>Poi ci sono storie più quotidiane ma ugualmente inquietanti, come quella del credit scoring usato nei colloqui di lavoro. Negli Stati Uniti (ma attenzione, la tendenza si sta diffondendo anche altrove), molte aziende ora stanno controllando il punteggio di affidabilità creditizia anche prima di assumere. Così, chi parte già da condizioni svantaggiate – magari per colpe non sue – si trova chiuso fuori dal mercato del lavoro. Il meccanismo, alla fine, è quello della doppia pena: povero perché non hai credito, senza credito perché sei povero, e intanto l’algoritmo, come un giudice invisibile, inchioda il futuro a un numero.</p><p>O’Neil ci mette in guardia: ogni volta che un modello <strong>decide sulla base di “proxy”</strong> (cioè sostituti di dati veri, come il codice postale al posto del rischio reale, o il test standardizzato al posto della qualità educativa), siamo in zona rossa. Nel baseball, ci ricorda, le statistiche funzionano perché ogni dato riflette azioni concrete (un punto segnato, una palla mancata). Ma quando si passa dalla palla al campo sociale, i proxy diventano pericolosi: il codice postale riflette la povertà e, di riflesso, la razza; i punteggi dei test riflettono il contesto familiare, non solo l’impegno. Così l’algoritmo, travestito da giudice imparziale, diventa lo <strong>specchio dei nostri pregiudizi</strong>.</p><p>Un altro mito che O’Neil smonta senza pietà è quello del “più dati uguale più verità”. Anzi, ci ricorda che <strong>ogni modello è una semplificazione</strong>, e che senza feedback, senza la capacità di correggersi quando sbaglia, rischia solo di cristallizzare gli errori. Prendi le classifiche delle università americane – veri e propri totem per studenti e famiglie, al punto che le strategie degli atenei vengono dettate da punteggi decisi da una redazione di rivista. E allora via a gonfiare le statistiche, tagliare i corsi meno redditizi, investire in palestre e campus di lusso per salire in classifica. Il vero senso dell’educazione? Quello rischia di perdersi per strada.</p><p>E poi c’è il mondo della pubblicità online, che di neutrale non ha nulla. O’Neil ci porta dietro le quinte delle università “for-profit”, quelle che campano reclutando studenti fragili – madri single, disoccupati, reduci di guerra – promettendo miraggi di successo e lasciandoli solo con una montagna di debiti. La pubblicità mirata, in questo contesto, diventa una macchina perfetta di propaganda e selezione delle vittime: gli algoritmi scelgono chi colpire sulla base di dati che nessuno controlla, chi sta in cima al sistema incassa, chi sta in basso paga il prezzo, e spesso nemmeno si accorge di essere stato preso di mira da una “macchina”.</p><p>Le stesse dinamiche si trovano nella giustizia: polizia predittiva, sistemi di valutazione del rischio di recidiva, sorveglianza capillare. Prendi PredPol, il software che promette di “prevenire il crimine” (a <a href="https://www.imdb.com/it/title/tt0181689/">Minority Report</a> ci stiamo arrivando) indirizzando le pattuglie dove il rischio è più alto. A parole, tutto neutrale. Ma nella pratica, chi viene sorvegliato di più è chi vive nei quartieri poveri, e così più polizia produce più segnalazioni, che producono più dati, che rafforzano la sorveglianza. Il rischio di automantenere un pregiudizio – razziale, sociale – è enorme. E peggio ancora, spesso la persona non sa nemmeno di essere giudicata da un algoritmo; non può difendersi, non può discutere, non può nemmeno sapere cosa l’ha condannata. È una sorta di tribunale segreto, in cui l’accusato non può parlare.</p><p>Nel lavoro, la musica non cambia. Dai test di personalità automatizzati per le assunzioni (che finiscono spesso per discriminare chi ha storie di malattia o semplicemente risponde fuori dagli schemi) ai software che programmano i turni di lavoro nei negozi, l’algoritmo diventa il nuovo portiere, spesso più severo e meno trasparente di quelli in carne e ossa. O’Neil racconta il caso di Kyle Behm, scartato sistematicamente da supermercati perché i test psicometrici lo bollavano come “inadatto”. Nessuno che possa spiegare o correggere. Eppure, dietro la facciata dell’imparzialità, si nascondono nuove forme di esclusione sociale: chi è già fragile rischia di rimanere tale a tempo indeterminato, chi è diverso dal “modello vincente” dell’azienda viene messo da parte senza diritto di replica.</p><p>Interessante è notare come queste forme di automazione colpiscano soprattutto i più deboli: i lavori a basso salario, i candidati meno istruiti, le minoranze. La promessa di una valutazione scientifica ed equa svanisce se il sistema premia solo chi già parte avvantaggiato. E chi pensa che basti “avere il giusto profilo” per essere al sicuro, forse dovrebbe chiedersi quanto sia giusto vivere in un mondo in cui il prossimo “update” dell’algoritmo potrebbe ribaltare tutto senza preavviso.</p><p>Nella sua opera O’Neil illustra come sul posto di lavoro, poi, la logica dell’ottimizzazione continua porta ad esempio a turni spezzettati, orari impossibili da conciliare con la vita familiare o stress cronico. Il termine “clopening”, usato per chi fa chiusura serale e apertura mattutina nello stesso locale, è ormai familiare a molti commessi e camerieri. Tutto questo per inseguire l’efficienza massima – ogni minuto, ogni ora deve produrre qualcosa – ma chi paga davvero sono le persone, che vedono evaporare la possibilità di organizzarsi, di prendersi cura dei figli, di vivere serenamente. I sistemi che dovrebbero aiutare finiscono per trasformarsi in strumenti di controllo e di precarietà.</p><p>Anche i "colletti bianchi" non sono immuni: software come quelli sviluppati da Cataphora (che analizzano email e scambi digitali per mappare l’innovazione e decidere chi licenziare) rischiano di ridurre la ricchezza umana a una manciata di dati quantitativi. E se il tuo contributo non si vede nel grafico, poco importa: puoi essere tagliato senza che nessuno sappia davvero cosa hai portato all’azienda. La tentazione di “<strong>scaricare la colpa sull’algoritmo</strong>” è forte, e per chi subisce il danno, è quasi impossibile reagire.</p><p>A livello di società, come ci racconta O’Neil nel contesto statunitense, il meccanismo si ripete anche nel settore finanziario. Il credito, un tempo assegnato a discrezione di funzionari spesso prevenuti, è stato “democratizzato” dal punteggio FICO, basato su dati oggettivi. Un progresso, finché non è arrivata la nuova generazione di “e-scores”, punteggi opachi costruiti su dati aggregati dai social, dagli acquisti online, dalla cronologia web. <strong>Nessuno sa davvero come funzionino</strong>, nessuno può correggere errori, e spesso si finisce per essere valutati sulla base di variabili che non hanno nulla a che vedere con il proprio merito individuale. Sembra quasi che il vecchio “redlining” sia tornato in versione digitale: se vivi in un certo quartiere, se hai certi amici, se non compri certi prodotti, rischi di essere escluso senza saperlo.</p><p>E mentre in Europa qualcosa si muove (il GDPR offre alcune tutele, benché perfettibili), negli Stati Uniti il mercato dei dati personali è ancora un far west: aziende che comprano e vendono profili comportamentali senza che tu possa dire la tua, errori che si propagano nei sistemi senza possibilità di rettifica, discriminazioni che passano inosservate perché “automatiche”. O’Neil mostra con esempi concreti come le conseguenze siano spesso grottesche: chi paga l’assicurazione auto può trovarsi premi maggiorati non per come guida, ma per il proprio credit score. Se sei povero, paghi di più, e la spirale della povertà si rafforza. Oppure pensiamo ai sensori di fitness e ai programmi di wellness aziendale: strumenti pensati per migliorare la salute finiscono per fornire alle aziende un tesoro di dati intimi, che potrebbero diventare un domani criteri di selezione per assunzioni e promozioni. Un “health score” troppo basso e rischi di essere scartato, magari senza nemmeno saperlo.</p><p>C’è poi il tema della cittadinanza e della democrazia. Sì, perché gli algoritmi non si fermano alla sfera privata: influenzano ciò che vediamo nei feed dei social, quali notizie ci raggiungono, come vengono indirizzate le campagne elettorali. Il caso dell’esperimento di Facebook – mostrare o meno il box “<em>Hai votato?</em>” agli utenti, spingendo (scientificamente!) migliaia di persone in più alle urne – è solo la punta dell’iceberg. Gia oggi viviamo gli effetti delle <strong>distorsioni </strong>sempre maggiorni a fini politici per mezzo di questi strumenti. Se la personalizzazione delle informazioni portasse a una società di “<strong>bolle informative</strong>” in cui ciascuno vive nella propria realtà parallela? Il rischio che la democrazia stessa sia manipolata da logiche algoritmiche invisibili è reale, e O’Neil suona l’allarme senza mezzi termini.</p><p>A questo punto, viene spontaneo chiedersi: <em>“Tutto questo riguarda davvero anche l’Italia, oppure è solo roba d’oltreoceano?”</em> Siamo meno avanti di Stati Uniti e Cina nell’automazione selvaggia delle decisioni, ma la nostra quotidianità sta già da tempo sperimentando forme più o meno occulte di <em data-start="451" data-end="475">“<strong>giudizio algoritmico</strong>”</em>.</p><p>Prendiamo il caso emblematico dell’INPS e del cosiddetto “algoritmo dei navigator” durante il Reddito di Cittadinanza: il sistema, pensato per abbinare offerte di lavoro ai beneficiari, ha mostrato limiti enormi nel valutare profili e opportunità, spesso producendo risultati casuali o insensati, tanto che la Corte dei Conti (2023) ha evidenziato le lacune del matching automatico nelle sue relazioni.</p><p><span style="font-weight: 400;">Sempre nel settore del lavoro una vicenda esemplare è quella di Deliveroo. La piattaforma di food delivery utilizzava un algoritmo (denominato </span>Frank<span style="font-weight: 400;">) per gestire le prenotazioni delle sessioni di lavoro dei rider, assegnando priorità in base a un punteggio reputazionale di </span>“affidabilità” e “partecipazione”<span style="font-weight: 400;">. Il funzionamento preciso del modello era opaco, l’azienda non ha divulgato i criteri, ricostruiti solo grazie alle testimonianze dei rider. In pratica, il sistema penalizzava in modo uniforme qualsiasi cancellazione tardiva di un turno, </span>senza considerare i motivi<span style="font-weight: 400;">. Ciò significava che un rider veniva declassato nel ranking anche se rinunciava a una consegna per causa di forza maggiore, ad esempio sciopero, guasto, incidente o un malessere. Trattando allo stesso modo assenze giustificate e non, l’algoritmo finiva per </span><strong>discriminare indirettamente</strong><span style="font-weight: 400;"> i lavoratori riducendo le loro opportunità di prenotare le fasce orarie più redditizie. Nel 2020 il Tribunale di Bologna ha riconosciuto questo effetto distorsivo</span><span style="font-weight: 400;"> condannando Deliveroo per condotta illegittima nei confronti dei rider.</span></p><p><span style="font-weight: 400;">Anche nel mondo della scuola italiana si è verificato un caso tipico di algoritmo dagli effetti perversi. Nel 2016, a seguito della riforma detta “Buona Scuola”, il Ministero dell’Istruzione ha utilizzato un sistema automatizzato per gestire la </span>mobilità di oltre 100 mila insegnanti<span style="font-weight: 400;"> su tutto il territorio nazionale. L’obiettivo era assegnare le sedi in base a punteggi di servizio e preferenze, ma il risultato è stato caotico: </span>migliaia di docenti con punteggi alti sono stati trasferiti lontano da casa<span style="font-weight: 400;">, spesso dal Sud al Nord, mentre cattedre più vicine venivano attribuite a colleghi con punteggi inferiori. L’“algoritmo impazzito”, come fu ribattezzato, presentava errori e criteri oscuri, generando proteste diffuse per le </span>ingiustizie e disagi familiari<span style="font-weight: 400;"> causati. In seguito, una sentenza del TAR del Lazio ha censurato duramente quel sistema, definendolo un </span><strong>“metodo orwelliano”</strong><span style="font-weight: 400;"> in cui una decisione così delicata era lasciata a un algoritmo non supervisionato. I giudici hanno evidenziato che il software operava in modo </span>confuso e lacunoso<span style="font-weight: 400;">, basato su dati inseriti male, e soprattutto che </span>non rispettava il merito.</p><p>In campo finanziario, gli algoritmi di credit scoring – usati da banche e finanziarie per decidere a chi concedere un prestito e a quali condizioni – possono generare <strong data-start="8263" data-end="8292">discriminazioni indirette</strong> verso determinate categorie. In Italia sono emerse evidenze di bias soprattutto nei confronti dei clienti immigrati. Studi recenti condotti anche dalla Banca d’Italia hanno rilevato che, <em>a parità di caratteristiche socio-economiche</em>, un richiedente straniero ha una probabilità di vedersi rifiutare un mutuo più alta di circa 2-3 punti percentuali rispetto a un pari profilo italiano. Inoltre, anche quando il finanziamento viene erogato o viene dimostrata una storia creditizia solida, ai clienti non nativi spesso tocca un tasso d’interesse leggermente superiore (in media pochi decimali in più) rispetto ai mutuatari italiani. Questa disparità suggerisce che gli algoritmi utilizzati incorporano variabili o dati proxy correlati con l’origine etnica/nazionale.</p><p><span style="font-weight: 400;">Anche nella pubblica amministrazione italiana si stanno affacciando sistemi di </span><strong>punteggio algoritmico dei cittadini</strong><span style="font-weight: 400;"> che hanno sollevato dibattito. Ispirandosi (inconsapevolmente) al discusso modello di “credito sociale” cinese, alcuni enti locali hanno proposto di premiare con punti i comportamenti virtuosi dei residenti. Ad esempio </span>Roma<span style="font-weight: 400;"> ha testato lo </span><i><span style="font-weight: 400;">Smart Citizen Wallet</span></i><span style="font-weight: 400;">, un portafoglio digitale dove i cittadini accumulano crediti se usano mezzi pubblici o riciclano correttamente; il </span>Comune di Bologna<span style="font-weight: 400;"> ha ipotizzato un sistema simile di </span>incentivi per chi adotta stili di vita “green”<span style="font-weight: 400;">, dalla mobilità sostenibile alla differenziata. Il caso più estremo è però quello di </span>Fidenza<span style="font-weight: 400;">: qui l’amministrazione ha introdotto un meccanismo a punti per gli inquilini delle </span>case popolari<span style="font-weight: 400;">, valutandone il comportamento sociale. I destinatari degli alloggi ottengono bonus o malus e </span>possono persino essere<strong> sfrattati se perdono tutti i punti</strong><span style="font-weight: 400;"> accumulati negativamente. L’intento dichiarato è responsabilizzare e incoraggiare il rispetto delle regole condominiali, ma una simile misura rischia di </span>colpire proprio i più fragili<span style="font-weight: 400;">. Iniziative del genere infatti </span>premiano chi può permettersi comportamenti “virtuosi”.<strong> </strong>Senza un opportuno <strong data-start="11889" data-end="11913">contesto di supporto</strong> (migliori trasporti pubblici, politiche sociali, dialogo con i cittadini), questo “<strong>rating civico</strong>” finisce per amplificare le disuguaglianze: per di più raccogliendo una mole di dati personali sui cittadini. Non sorprende quindi che il Garante Privacy e varie associazioni abbiano espresso allarme su questi progetti, evidenziando il rischio di derive antidemocratiche e chiedendone una revisione prima di una loro eventuale attuazione su larga scala.</p><p>A leggere tutto questo, verrebbe da cedere allo sconforto. E invece il libro di O’Neil è, paradossalmente, un invito a reagire. Non a demonizzare la tecnologia, sia chiaro – lei stessa, da matematica, ama la disciplina e sa quanto bene può fare quando è usata con consapevolezza e responsabilità. Il punto è non delegare alla matematica (o meglio, a chi la scrive in codice) il compito di decidere cosa è giusto e cosa no. Serve trasparenza, sì, ma serve soprattutto <strong>coscienza collettiva</strong>. Perché ogni algoritmo <strong>nasce da scelte umane</strong> – da un’idea di successo, da una definizione di “merito”, da una scala di valori. Se quei valori sono mal definiti o ciechi rispetto alle diversità reali, allora anche il modello più sofisticato diventa una gabbia, e la matematica smette di essere un’amica della giustizia.</p><p>Forse è il momento di pensare a una <strong>“deontologia degli algoritmi”</strong>, come suggerisce O’Neil: un codice etico per chi scrive, addestra, implementa sistemi che incidono sulle vite delle persone. Immagina se ogni data scientist dovesse giurare, come un medico, di “non nuocere”. Sì, sembra idealistico, ma l’alternativa è rassegnarsi a una società in cui nessuno sa davvero perché le cose gli capitano – se hai ottenuto un mutuo, se sei stato scartato da un lavoro, se hai ricevuto una pubblicità mirata proprio nel giorno di maggiore fragilità.</p><p>Certo, la soluzione non può essere solo individuale. O’Neil fa notare che senza organismi di controllo pubblici – senza “auditor degli algoritmi”, senza regole chiare su cosa si può e non si può automatizzare – rischiamo di inseguire i problemi senza mai affrontarli davvero. L’Europa con l'AI Act si sta provando ad andare in questa direzione, ma sarebbe ora di pensare a istituzioni che vigilino sui modelli ad alto impatto sociale con la stessa serietà con cui si controllano i farmaci o le banche.</p><p>Una nota finale, ma che finale non è: l’arma di distruzione matematica, in fondo, non è altro che la matematica privata del suo feedback umano, cieca al contesto, sorda ai casi particolari, indifferente alla storia. Per renderla innocua – o meglio, per trasformarla in un’alleata – <strong>serve reintegrare nel processo decisionale le persone</strong>, le loro voci, la possibilità di spiegare e correggere. Non tutto può essere ridotto a numero, e la democrazia si misura anche da quanto riesce a difendere questa irriducibile complessità.</p><p>Non ci sono risposte facili, ma c’è una certezza: <strong>il dibattito va portato fuori dai circoli degli esperti, reso materia di educazione civica e di conversazione pubblica</strong>. Gli algoritmi sono troppo importanti per essere lasciati solo ai tecnici, troppo pervasivi per essere ignorati. Se non vogliamo che diventino la nuova burocrazia inespugnabile del XXI secolo, serve un nuovo patto tra scienza, società e diritti umani. E serve subito.</p><p class="align-center">«<em>i modelli non sono altro che opinioni scritte nel linguaggio della matematica</em>» cit.</p><p>Forse la vera domanda non è se gli algoritmi siano “<em>buoni</em>” o “<em>cattivi</em>”, ma se siamo ancora capaci – come società – di discuterne insieme, di <strong>pretendere che restino strumenti al nostro servizio</strong>, e non il contrario. O’Neil con le sue indagini accende la luce in una stanza che sembrava perfettamente in ordine, e ci mostra la polvere sotto il tappeto. Ora, a noi decidere cosa farne. </p><hr><p>Questo post è parte della rubrica <strong><a href="https://pianetararo.org/traiettorie/">TrAIettorie</a></strong> di cui potete trovare l'indice completo <a href="https://pianetararo.org/tags/traiettorie/">qui</a>.</p></div><footer class="content__footer"><div class="entry-wrapper"><div class="content__actions"><ul class="content__tag"><li><a href="https://pianetararo.org/tags/traiettorie/">TRAIETTORIE</a></li></ul><div class="content__share"><button class="btn--icon content__share-button js-content__share-button"><svg width="20" height="20" aria-hidden="true"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#share"></use></svg> <span>Share It</span></button><div class="content__share-popup js-content__share-popup"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fpianetararo.org%2Farmi-di-distruzione-matematica-cathy-oneil%2F" class="js-share facebook" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#facebook"/></svg> <span>Facebook</span> </a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fpianetararo.org%2Farmi-di-distruzione-matematica-cathy-oneil%2F" class="js-share linkedin" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#linkedin"/></svg> <span>LinkedIn</span> </a><a href="https://api.whatsapp.com/send?text=Armi%20di%20distruzione%20matematica%20(Cathy%20O%E2%80%99Neil) https%3A%2F%2Fpianetararo.org%2Farmi-di-distruzione-matematica-cathy-oneil%2F" class="js-share whatsapp" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#whatsapp"/></svg> <span>WhatsApp</span></a></div></div></div></div></footer></article></main><footer class="footer"><div class="wrapper"><div class="footer__copyright"><div class="block-footer-fl" style="width: 100%; height: 100%; , max-width: 100%;"><p class="align-center" style="font-size: x-small;"><span style="color: #44684b;">Pianetararo associazione culturale</span><br><span style="color: #44684b;">CF: 04015870365</span><br><span style="color: #44684b;">info@pianetararo.org</span><br><span style="color: #44684b;">Pianetararo è un associazione senza scopo di lucro e partecipa al programma "Google for Non profits".</span></p><p class="align-center" style="font-size: x-small;"><span style="color: #44684b;"><a href="https://pianetararo.org/privacy-and-cookie/" title="Privacy &amp; Cookie policy" style="color: #44684b;">Privacy &amp; Cookie policy</a></span></p></div></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg width="20" height="20"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#toparrow"/></svg></button></div></footer><script defer="defer" src="https://pianetararo.org/assets/js/scripts.min.js?v=700105c316933a8202041b6415abb233"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script><div class="pcb" data-behaviour="badge" data-behaviour-link="#cookie-settings" data-revision="1" data-config-ttl="90" data-debug-mode="false"><div role="dialog" aria-modal="true" aria-hidden="true" aria-labelledby="pcb-title" aria-describedby="pcb-txt" class="pcb__banner"><div class="pcb__inner"><div id="pcb-title" role="heading" aria-level="2" class="pcb__title">This website uses cookies</div><div id="pcb-txt" class="pcb__txt">Select which cookies to opt-in to via the checkboxes below; our website uses cookies to examine site traffic and user activity while on our site, for marketing, and to provide social media functionality. <a href="https://pianetararo.org/privacy-and-cookie/">More details...</a></div><div class="pcb__buttons"><button type="button" class="pcb__btn pcb__btn--solid pcb__btn--accept">Accept all</button></div></div></div><button class="pcb__badge" aria-label="Cookie Policy" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" width="40" height="40" viewBox="0 0 23 23" fill="currentColor"><path d="M21.41 12.71c-.08-.01-.15 0-.22 0h-.03c-.03 0-.05 0-.08.01-.07 0-.13.01-.19.04-.52.21-1.44.19-2.02-.22-.44-.31-.65-.83-.62-1.53a.758.758 0 0 0-.27-.61.73.73 0 0 0-.65-.14c-1.98.51-3.49.23-4.26-.78-.82-1.08-.73-2.89.24-4.49.14-.23.14-.52 0-.75a.756.756 0 0 0-.67-.36c-.64.03-1.11-.1-1.31-.35-.19-.26-.13-.71-.01-1.29.04-.18.06-.38.03-.59-.05-.4-.4-.7-.81-.66C5.1 1.54 1 6.04 1 11.48 1 17.28 5.75 22 11.6 22c5.02 0 9.39-3.54 10.39-8.42.08-.4-.18-.78-.58-.87Zm-9.81 7.82c-5.03 0-9.12-4.06-9.12-9.06 0-4.34 3.05-8 7.25-8.86-.08.7.05 1.33.42 1.81.24.32.66.67 1.38.84-.76 1.86-.65 3.78.36 5.11.61.81 2.03 2 4.95 1.51.18.96.71 1.54 1.18 1.87.62.43 1.38.62 2.1.62.05 0 .09 0 .13-.01-1.23 3.64-4.7 6.18-8.64 6.18ZM13 17c0 .55-.45 1-1 1s-1-.45-1-1 .45-1 1-1 1 .45 1 1Zm5.29-12.3a.99.99 0 0 1-.29-.71c0-.55.45-.99 1-.99a1 1 0 0 1 .71.3c.19.19.29.44.29.71 0 .55-.45.99-1 .99a1 1 0 0 1-.71-.3ZM9 13.5c0 .83-.67 1.5-1.5 1.5S6 14.33 6 13.5 6.67 12 7.5 12s1.5.67 1.5 1.5Zm3.25.81a.744.744 0 0 1-.06-1.05c.28-.32.75-.34 1.05-.06.31.28.33.75.05 1.06-.15.16-.35.25-.56.25-.18 0-.36-.06-.5-.19ZM8.68 7.26c.41.37.44 1 .07 1.41-.2.22-.47.33-.75.33a.96.96 0 0 1-.67-.26c-.41-.37-.44-1-.07-1.41.37-.42 1-.45 1.41-.08Zm11.48 1.88c.18-.19.52-.19.7 0 .05.04.09.1.11.16.03.06.04.12.04.19 0 .13-.05.26-.15.35-.09.1-.22.15-.35.15s-.26-.05-.35-.15a.355.355 0 0 1-.11-.16.433.433 0 0 1-.04-.19c0-.13.05-.26.15-.35Zm-4.93-1.86a.75.75 0 1 1 1.059-1.06.75.75 0 0 1-1.059 1.06Z"/></svg></button></div><script>(function(win) {
    if (!document.querySelector('.pcb')) {
        return;
    }

    var cbConfig = {
        behaviour: document.querySelector('.pcb').getAttribute('data-behaviour'),
        behaviourLink: document.querySelector('.pcb').getAttribute('data-behaviour-link'),
        revision: document.querySelector('.pcb').getAttribute('data-revision'),
        configTTL: parseInt(document.querySelector('.pcb').getAttribute('data-config-ttl'), 10),
        debugMode: document.querySelector('.pcb').getAttribute('data-debug-mode') === 'true',
        initialState: null,
        initialLsState: null,
        previouslyAccepted: []
    };

    var cbUI = {
        wrapper: document.querySelector('.pcb'),
        banner: {
            element: null,
            btnAccept: null,
            btnReject: null,
            btnConfigure: null
        },
        popup: {
            element: null,
            btnClose: null,
            btnSave: null,
            btnAccept: null,
            btnReject: null,
            checkboxes: null,
        },
        overlay: null,
        badge: null,
        blockedScripts: document.querySelectorAll('script[type^="gdpr-blocker/"]'),
        triggerLinks: cbConfig.behaviourLink ? document.querySelectorAll('a[href*="' + cbConfig.behaviourLink + '"]') : null
    };

    function initUI () {
        // setup banner elements
        cbUI.banner.element = cbUI.wrapper.querySelector('.pcb__banner');
        cbUI.banner.btnAccept = cbUI.banner.element.querySelector('.pcb__btn--accept');
        cbUI.banner.btnReject = cbUI.banner.element.querySelector('.pcb__btn--reject');
        cbUI.banner.btnConfigure = cbUI.banner.element.querySelector('.pcb__btn--configure');

        // setup popup elements
        if (cbUI.wrapper.querySelector('.pcb__popup')) {
            cbUI.popup.element = cbUI.wrapper.querySelector('.pcb__popup');
            cbUI.popup.btnClose = cbUI.wrapper.querySelector('.pcb__popup__close');
            cbUI.popup.btnSave = cbUI.popup.element.querySelector('.pcb__btn--save');
            cbUI.popup.btnAccept = cbUI.popup.element.querySelector('.pcb__btn--accept');
            cbUI.popup.btnReject = cbUI.popup.element.querySelector('.pcb__btn--reject');
            cbUI.popup.checkboxes = cbUI.popup.element.querySelector('input[type="checkbox"]');
            // setup overlay
            cbUI.overlay = cbUI.wrapper.querySelector('.pcb__overlay');
        }

        cbUI.badge = cbUI.wrapper.querySelector('.pcb__badge');

        if (cbConfig.behaviour.indexOf('link') > -1) {
            for (var i = 0; i < cbUI.triggerLinks.length; i++) {
                cbUI.triggerLinks[i].addEventListener('click', function(e) {
                    e.preventDefault();
                    showBannerOrPopup();
                });
            }
        }
    }

    function initState () {
        var lsKeyName = getConfigName();
        var currentConfig = localStorage.getItem(lsKeyName);
        var configIsFresh = checkIfConfigIsFresh();

        if (!configIsFresh || currentConfig === null) {
            if (cbConfig.debugMode) {
                console.log('🍪 Config not found, or configuration expired');
            }

            if (window.publiiCBGCM) {
                gtag('consent', 'default', {
                    'ad_storage': window.publiiCBGCM.defaultState.ad_storage ? 'granted' : 'denied',
                    'ad_personalization': window.publiiCBGCM.defaultState.ad_personalization ? 'granted' : 'denied',
                    'ad_user_data': window.publiiCBGCM.defaultState.ad_user_data ? 'granted' : 'denied',
                    'analytics_storage': window.publiiCBGCM.defaultState.analytics_storage ? 'granted' : 'denied',
                    'personalization_storage': window.publiiCBGCM.defaultState.personalization_storage ? 'granted' : 'denied',
                    'functionality_storage': window.publiiCBGCM.defaultState.functionality_storage ? 'granted' : 'denied',
                    'security_storage': window.publiiCBGCM.defaultState.security_storage ? 'granted' : 'denied'
                });  
                
                if (cbConfig.debugMode) {
                    console.log('🍪 GCMv2 DEFAULT STATE: ' + JSON.stringify({
                        'ad_storage': window.publiiCBGCM.defaultState.ad_storage ? 'granted' : 'denied',
                        'ad_personalization': window.publiiCBGCM.defaultState.ad_personalization ? 'granted' : 'denied',
                        'ad_user_data': window.publiiCBGCM.defaultState.ad_user_data ? 'granted' : 'denied',
                        'analytics_storage': window.publiiCBGCM.defaultState.analytics_storage ? 'granted' : 'denied',
                        'personalization_storage': window.publiiCBGCM.defaultState.personalization_storage ? 'granted' : 'denied',
                        'functionality_storage': window.publiiCBGCM.defaultState.functionality_storage ? 'granted' : 'denied',
                        'security_storage': window.publiiCBGCM.defaultState.security_storage ? 'granted' : 'denied'
                    }));
                }
            }

            showBanner();
        } else if (typeof currentConfig === 'string') {
            if (cbConfig.debugMode) {
                console.log('🍪 Config founded');
            }

            cbConfig.initialLsState = currentConfig.split(',');

            if (window.publiiCBGCM) {
                gtag('consent', 'default', {
                    'ad_storage': getDefaultConsentState(currentConfig, 'ad_storage'),
                    'ad_personalization': getDefaultConsentState(currentConfig, 'ad_personalization'),
                    'ad_user_data': getDefaultConsentState(currentConfig, 'ad_user_data'),
                    'analytics_storage': getDefaultConsentState(currentConfig, 'analytics_storage'),
                    'personalization_storage': getDefaultConsentState(currentConfig, 'personalization_storage'),
                    'functionality_storage': getDefaultConsentState(currentConfig, 'functionality_storage'),
                    'security_storage': getDefaultConsentState(currentConfig, 'security_storage')
                });
                
                if (cbConfig.debugMode) {
                    console.log('🍪 GCMv2 DEFAULT STATE: ' + JSON.stringify({
                        'ad_storage': getDefaultConsentState(currentConfig, 'ad_storage'),
                        'ad_personalization': getDefaultConsentState(currentConfig, 'ad_personalization'),
                        'ad_user_data': getDefaultConsentState(currentConfig, 'ad_user_data'),
                        'analytics_storage': getDefaultConsentState(currentConfig, 'analytics_storage'),
                        'personalization_storage': getDefaultConsentState(currentConfig, 'personalization_storage'),
                        'functionality_storage': getDefaultConsentState(currentConfig, 'functionality_storage'),
                        'security_storage': getDefaultConsentState(currentConfig, 'security_storage')
                    }));
                }
            }

            showBadge();

            if (cbUI.popup.element) {
                var allowedGroups = currentConfig.split(',');
                var checkedCheckboxes = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');

                for (var j = 0; j < checkedCheckboxes.length; j++) {
                    var name = checkedCheckboxes[j].getAttribute('data-group-name');

                    if (name && name !== '-' && allowedGroups.indexOf(name) === -1) {
                        checkedCheckboxes[j].checked = false;
                    }
                }

                for (var i = 0; i < allowedGroups.length; i++) {
                    var checkbox = cbUI.popup.element.querySelector('input[type="checkbox"][data-group-name="' + allowedGroups[i] + '"]');

                    if (checkbox) {
                        checkbox.checked = true;
                    }

                    allowCookieGroup(allowedGroups[i]);
                }
            }
        }

        setTimeout(function () {
            cbConfig.initialState = getInitialStateOfConsents();
        }, 0);
    }

    function checkIfConfigIsFresh () {
        var lastConfigSave = localStorage.getItem('publii-gdpr-cookies-config-save-date');

        if (lastConfigSave === null) {
            return false;
        }

        lastConfigSave = parseInt(lastConfigSave, 10);

        if (lastConfigSave === 0) {
            return true;
        }

        if (+new Date() - lastConfigSave < cbConfig.configTTL * 24 * 60 * 60 * 1000) {
            return true;
        }

        return false;
    }

    function getDefaultConsentState (currentConfig, consentGroup) {
        let configGroups = currentConfig.split(',');

        for (let i = 0; i < configGroups.length; i++) {
            let groupName = configGroups[i];
            let group = window.publiiCBGCM.groups.find(group => group.cookieGroup === groupName);

            if (group && group[consentGroup]) {
                return 'granted';
            }
        }  
        
        if (window.publiiCBGCM.defaultState[consentGroup]) {
            return 'granted'; 
        }
        
        return 'denied';
    }

    function initBannerEvents () {
        cbUI.banner.btnAccept.addEventListener('click', function (e) {
            e.preventDefault();
            acceptAllCookies('banner');
            showBadge();
        }, false);

        if (cbUI.banner.btnReject) {
            cbUI.banner.btnReject.addEventListener('click', function (e) {
                e.preventDefault();
                rejectAllCookies();
                showBadge();
            }, false);
        }

        if (cbUI.banner.btnConfigure) {
            cbUI.banner.btnConfigure.addEventListener('click', function (e) {
                e.preventDefault();
                hideBanner();
                showAdvancedPopup();
                showBadge();
            }, false);
        }
    }

    function initPopupEvents () {
        if (!cbUI.popup.element) {
            return;
        }

        cbUI.overlay.addEventListener('click', function (e) {
            hideAdvancedPopup();
        }, false);

        cbUI.popup.element.addEventListener('click', function (e) {
            e.stopPropagation();
        }, false);

        cbUI.popup.btnAccept.addEventListener('click', function (e) {
            e.preventDefault();
            acceptAllCookies('popup');
        }, false);

        cbUI.popup.btnReject.addEventListener('click', function (e) {
            e.preventDefault();
            rejectAllCookies();
        }, false);

        cbUI.popup.btnSave.addEventListener('click', function (e) {
            e.preventDefault();
            saveConfiguration();
        }, false);

        cbUI.popup.btnClose.addEventListener('click', function (e) {
            e.preventDefault();
            hideAdvancedPopup();
        }, false);
    }

    function initBadgeEvents () {
        if (!cbUI.badge) {
            return;
        }

        cbUI.badge.addEventListener('click', function (e) {
            showBannerOrPopup();
        }, false);
    }

    initUI();
    initState();
    initBannerEvents();
    initPopupEvents();
    initBadgeEvents();

    /**
     * API
     */
    function addScript (src, inline) {
        var newScript = document.createElement('script');

        if (src) {
            newScript.setAttribute('src', src);
        }

        if (inline) {
            newScript.text = inline;
        }

        document.body.appendChild(newScript);
    }

    function allowCookieGroup (allowedGroup) {
        var scripts = document.querySelectorAll('script[type="gdpr-blocker/' + allowedGroup + '"]');
        cbConfig.previouslyAccepted.push(allowedGroup);
    
        for (var j = 0; j < scripts.length; j++) {
            addScript(scripts[j].src, scripts[j].text);
        }

        var groupEvent = new Event('publii-cookie-banner-unblock-' + allowedGroup);
        document.body.dispatchEvent(groupEvent);
        unlockEmbeds(allowedGroup);

        if (cbConfig.debugMode) {
            console.log('🍪 Allowed group: ' + allowedGroup);
        }

        if (window.publiiCBGCM && cbConfig.initialLsState.indexOf(allowedGroup) === -1) {
            let consentResult = {};
            let group = window.publiiCBGCM.groups.find(group => group.cookieGroup === allowedGroup);

            if (group) {
                let foundSomeConsents = false;

                Object.keys(group).forEach(key => {
                    if (key !== 'cookieGroup' && group[key] === true) {
                        consentResult[key] = 'granted';
                        foundSomeConsents = true;
                    }
                });

                if (foundSomeConsents) {
                    gtag('consent', 'update', consentResult);   

                    if (cbConfig.debugMode) {
                        console.log('🍪 GCMv2 UPDATE: ' + JSON.stringify(consentResult));
                    }
                }
            }
        }
    }

    function showBannerOrPopup () {
        if (cbUI.popup.element) {
            showAdvancedPopup();
        } else {
            showBanner();
        }
    }

    function showAdvancedPopup () {
        cbUI.popup.element.classList.add('is-visible');
        cbUI.overlay.classList.add('is-visible');
        cbUI.popup.element.setAttribute('aria-hidden', 'false');
        cbUI.overlay.setAttribute('aria-hidden', 'false');
    }

    function hideAdvancedPopup () {
        cbUI.popup.element.classList.remove('is-visible');
        cbUI.overlay.classList.remove('is-visible');
        cbUI.popup.element.setAttribute('aria-hidden', 'true');
        cbUI.overlay.setAttribute('aria-hidden', 'true');
    }

    function showBanner () {
        cbUI.banner.element.classList.add('is-visible');
        cbUI.banner.element.setAttribute('aria-hidden', 'false');
    }

    function hideBanner () {
        cbUI.banner.element.classList.remove('is-visible');
        cbUI.banner.element.setAttribute('aria-hidden', 'true');
    }

    function showBadge () {
        if (!cbUI.badge) {
            return;
        }

        cbUI.badge.classList.add('is-visible');
        cbUI.badge.setAttribute('aria-hidden', 'false');
    }

    function getConfigName () {
        var lsKeyName = 'publii-gdpr-allowed-cookies';

        if (cbConfig.revision) {
            lsKeyName = lsKeyName + '-v' + parseInt(cbConfig.revision, 10);
        }

        return lsKeyName;
    }

    function storeConfiguration (allowedGroups) {
        var lsKeyName = getConfigName();
        var dataToStore = allowedGroups.join(',');
        localStorage.setItem(lsKeyName, dataToStore);

        if (cbConfig.configTTL === 0) {
            localStorage.setItem('publii-gdpr-cookies-config-save-date', 0);

            if (cbConfig.debugMode) {
                console.log('🍪 Store never expiring configuration');
            }
        } else {
            localStorage.setItem('publii-gdpr-cookies-config-save-date', +new Date());
        }
    }

    function getInitialStateOfConsents () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        if (cbConfig.debugMode) {
            console.log('🍪 Initial state: ' + groups.join(', '));
        }

        return groups;
    }

    function getCurrentStateOfConsents () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        if (cbConfig.debugMode) {
            console.log('🍪 State to save: ' + groups.join(', '));
        }

        return groups;
    }

    function getAllGroups () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        return groups;
    }

    function acceptAllCookies (source) {
        var groupsToAccept = getAllGroups();
        storeConfiguration(groupsToAccept);

        for (var i = 0; i < groupsToAccept.length; i++) {
            var group = groupsToAccept[i];

            if (cbConfig.initialState.indexOf(group) > -1 || cbConfig.previouslyAccepted.indexOf(group) > -1) {
                if (cbConfig.debugMode) {
                    console.log('🍪 Skip previously activated group: ' + group);
                }

                continue;
            }

            allowCookieGroup(group);
        }

        if (cbUI.popup.element) {
            var checkboxesToCheck = cbUI.popup.element.querySelectorAll('input[type="checkbox"]');

            for (var j = 0; j < checkboxesToCheck.length; j++) {
                checkboxesToCheck[j].checked = true;
            }
        }

        if (cbConfig.debugMode) {
            console.log('🍪 Accept all cookies: ', groupsToAccept.join(', '));
        }

        if (source === 'popup') {
            hideAdvancedPopup();
        } else if (source === 'banner') {
            hideBanner();
        }
    }

    function rejectAllCookies () {
        if (cbConfig.debugMode) {
            console.log('🍪 Reject all cookies');
        }

        storeConfiguration([]);
        setTimeout(function () {
            window.location.reload();
        }, 100);
    }

    function saveConfiguration () {
        var groupsToAccept = getCurrentStateOfConsents();
        storeConfiguration(groupsToAccept);

        if (cbConfig.debugMode) {
            console.log('🍪 Save new config: ', groupsToAccept.join(', '));
        }

        if (reloadIsNeeded(groupsToAccept)) {
            setTimeout(function () {
                window.location.reload();
            }, 100);
            return;
        }

        for (var i = 0; i < groupsToAccept.length; i++) {
            var group = groupsToAccept[i];

            if (cbConfig.initialState.indexOf(group) > -1 || cbConfig.previouslyAccepted.indexOf(group) > -1) {
                if (cbConfig.debugMode) {
                    console.log('🍪 Skip previously activated group: ' + group);
                }

                continue;
            }

            allowCookieGroup(group);
        }

        hideAdvancedPopup();
    }

    function reloadIsNeeded (groupsToAccept) {
        // check if user rejected consent for initial groups
        var initialGroups = cbConfig.initialState;
        var previouslyAcceptedGroups = cbConfig.previouslyAccepted;
        var groupsToCheck = initialGroups.concat(previouslyAcceptedGroups);

        for (var i = 0; i < groupsToCheck.length; i++) {
            var groupToCheck = groupsToCheck[i];

            if (groupToCheck !== '' && groupsToAccept.indexOf(groupToCheck) === -1) {
                if (cbConfig.debugMode) {
                    console.log('🍪 Reload is needed due lack of: ', groupToCheck);
                }

                return true;
            }
        }

        return false;
    }

    function unlockEmbeds (cookieGroup) {
        var iframesToUnlock = document.querySelectorAll('.pec-wrapper[data-consent-group-id="' + cookieGroup + '"]');

        for (var i = 0; i < iframesToUnlock.length; i++) {
            var iframeWrapper = iframesToUnlock[i];
            iframeWrapper.querySelector('.pec-overlay').classList.remove('is-active');
            iframeWrapper.querySelector('.pec-overlay').setAttribute('aria-hidden', 'true');
            var iframe = iframeWrapper.querySelector('iframe');
            iframe.setAttribute('src', iframe.getAttribute('data-consent-src'));
        }
    }

    win.publiiEmbedConsentGiven = function (cookieGroup) {
        // it will unlock embeds
        allowCookieGroup(cookieGroup);

        var checkbox = cbUI.popup.element.querySelector('input[type="checkbox"][data-group-name="' + cookieGroup + '"]');

        if (checkbox) {
            checkbox.checked = true;
        }

        var groupsToAccept = getCurrentStateOfConsents();
        storeConfiguration(groupsToAccept);

        if (cbConfig.debugMode) {
            console.log('🍪 Save new config: ', groupsToAccept.join(', '));
        }
    }
})(window);</script></body></html>