<!DOCTYPE html><html lang="it"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Come funzionano le AI che ci scrivono - pianetararo</title><meta name="description" content="Interagire con un chatbot ‚Äì che sia ChatGPT, Gemini, Claude o uno dei tanti assistenti virtuali ‚Äì ormai √® quasi normale, anzi sta rimpiazzando velocemente la classica ricerca sul web. Chiediamo a un‚ÄôAI di cercare qualcosa, scriverci il riassunto di un testo, una poesia in&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><script type="text/javascript" async src="https://www.googletagmanager.com/gtag/js?id=G-EDXZ56Y7D7"></script><script type="text/javascript">window.dataLayer = window.dataLayer || [];
				  function gtag(){dataLayer.push(arguments);}
				  gtag('js', new Date());
				  gtag('config', 'G-EDXZ56Y7D7' );</script><link rel="canonical" href="https://pianetararo.org/come-funzionano-le-ia-che-ci-scrivono/"><link rel="alternate" type="application/atom+xml" href="https://pianetararo.org/feed.xml"><link rel="alternate" type="application/json" href="https://pianetararo.org/feed.json"><meta property="og:title" content="Come funzionano le AI che ci scrivono"><meta property="og:image" content="https://pianetararo.org/media/posts/22/penna-formula.png"><meta property="og:image:width" content="1536"><meta property="og:image:height" content="1024"><meta property="og:site_name" content="pianetararo"><meta property="og:description" content="Interagire con un chatbot ‚Äì che sia ChatGPT, Gemini, Claude o uno dei tanti assistenti virtuali ‚Äì ormai √® quasi normale, anzi sta rimpiazzando velocemente la classica ricerca sul web. Chiediamo a un‚ÄôAI di cercare qualcosa, scriverci il riassunto di un testo, una poesia in&hellip;"><meta property="og:url" content="https://pianetararo.org//come-funzionano-le-ia-che-ci-scrivono/"><meta property="og:type" content="article"><link rel="stylesheet" href="https://pianetararo.org/assets/css/style.css?v=812e0178178abea4ea9399c6007c2ff4"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://pianetararo.org/come-funzionano-le-ia-che-ci-scrivono/"},"headline":"Come funzionano le AI che ci scrivono","datePublished":"2025-06-01T20:00+02:00","dateModified":"2025-06-11T13:42+02:00","image":{"@type":"ImageObject","url":"https://pianetararo.org/media/posts/22/penna-formula.png","height":1024,"width":1536},"description":"Interagire con un chatbot ‚Äì che sia ChatGPT, Gemini, Claude o uno dei tanti assistenti virtuali ‚Äì ormai √® quasi normale, anzi sta rimpiazzando velocemente la classica ricerca sul web. Chiediamo a un‚ÄôAI di cercare qualcosa, scriverci il riassunto di un testo, una poesia in&hellip;","author":{"@type":"Person","name":"Pianetararo Associazione Culturale","url":"https://pianetararo.org/authors/pianetararo-associazione-culturale/"},"publisher":{"@type":"Organization","name":"Pianetararo Associazione Culturale","logo":{"@type":"ImageObject","url":"https://pianetararo.org/media/website/PIANETARARO-1-1-1.svg","height":375,"width":375}}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><header class="top js-header"><a class="logo" href="https://pianetararo.org/"><img src="https://pianetararo.org/media/website/PIANETARARO-1-1-1.svg" alt="pianetararo" width="375" height="375"></a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="https://pianetararo.org/test-2/" title="pianetararo" target="_self">Chi siamo</a></li><li><a href="https://pianetararo.org/traiettorie/" target="_self">TrAIettorie</a></li><li class="has-submenu"><a href="https://pianetararo.org/frammenti25intro/" title="frammenti25" target="_self" aria-haspopup="true">FRAMMENTI#25</a><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/" target="_self">FRAMMENTI DIGITAL - Come funziona ?</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/1-il-pozzo-di-san-patrizio/" target="_self">#1 - Gennaio</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/2-la-rocca-di-calascio/" target="_self">#2 - Febbraio</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/3-il-palio-delle-rane/" target="_self">#3 - Marzo</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/4-il-carnevale-di-mamoiada/" target="_self">#4 - Aprile</a></li><li><a href="https://pianetararo.org/frammenti25intro/frammenti25/5-il-castello-di-montebello-e-la-dama-bianca/" target="_self">#5 - Maggio</a></li></ul></li><li><a href="https://pianetararo.org/stradora/" title="STRADORA" target="_self">STRADORA</a></li><li><a href="https://pianetararo.org/pensieri/" title="PENSIERI" target="_self">Pensieri</a></li><li><a href="https://pianetararo.org/tags/" target="_self">Tags</a></li></ul></nav></header><main class="post"><article class="content"><div class="hero"><header class="hero__content"><div class="wrapper"><h1>Come funzionano le AI che ci scrivono</h1><div class="feed__meta content__meta"><time datetime="2025-06-01T20:00" class="feed__date">1 giu 2025</time></div></div></header><figure class="hero__image"><div class="hero__image-wrapper"><img src="https://pianetararo.org/media/posts/22/penna-formula.png" srcset="https://pianetararo.org/media/posts/22/responsive/penna-formula-xs.png 640w, https://pianetararo.org/media/posts/22/responsive/penna-formula-sm.png 768w, https://pianetararo.org/media/posts/22/responsive/penna-formula-md.png 1024w, https://pianetararo.org/media/posts/22/responsive/penna-formula-lg.png 1366w, https://pianetararo.org/media/posts/22/responsive/penna-formula-xl.png 1600w, https://pianetararo.org/media/posts/22/responsive/penna-formula-2xl.png 1920w" sizes="88vw" loading="eager" height="1024" width="1536" alt=""></div></figure></div><div class="entry-wrapper content__entry"><h1>¬†</h1><p><span style="font-weight: 400;">Interagire con un chatbot ‚Äì che sia ChatGPT, Gemini, Claude o uno dei tanti assistenti virtuali ‚Äì ormai √® quasi normale, anzi sta rimpiazzando velocemente la classica ricerca sul web. Chiediamo a un‚Äô</span>AI<span style="font-weight: 400;"> di cercare qualcosa, scriverci il riassunto di un testo, una poesia in rima o un consiglio per la cena, e quella risponde in italiano fluente e creativo. A volte sembra di parlare con un piccolo scrittore artificiale dalla fantasia infinita. </span><strong>Ma come fa, davvero, un modello linguistico a generare queste risposte?</strong><span style="font-weight: 400;"> Sta </span><strong>pensando</strong><span style="font-weight: 400;"> e </span><i><span style="font-weight: 400;">capendo</span></i><span style="font-weight: 400;"> la domanda come farebbe una persona? Risposta veloce: No. E capire </span><i><span style="font-weight: 400;">come funziona sul serio</span></i><span style="font-weight: 400;"> un LLM ‚Äì acronimo di </span>Large Language Model<span style="font-weight: 400;">, ovvero modello linguistico di grandi dimensioni ‚Äì √® importante per sfatare alcuni miti e usarlo al meglio.¬†</span></p><p><span style="font-weight: 400;">Questo articolo prover√† a fare chiarezza, semplificando, su cosa avviene </span><i><span style="font-weight: 400;">dietro le quinte</span></i><span style="font-weight: 400;"> quando una di queste IA elabora una domanda e produce una risposta. Con esempi semplici (anche fiabeschi!) vedremo passo passo </span><strong>come √® fatto internamente un LLM, come funziona e come genera testo</strong><span style="font-weight: 400;">, toccando concetti chiave e termini come </span><i><span style="font-weight: 400;">tokenizzazione</span></i><span style="font-weight: 400;">, </span><i><span style="font-weight: 400;">embedding</span></i><span style="font-weight: 400;"> e </span><i><span style="font-weight: 400;">trasformatori</span></i><span style="font-weight: 400;">. Scopriremo inoltre perch√© un LLM, per quanto sofisticato, </span><strong>non √® una mente cosciente</strong><span style="font-weight: 400;"> n√© infallibile, e come conoscere la sua struttura ci aiuta a usarlo in modo pi√π consapevole, apprezzandone le potenzialit√† senza dimenticarne i limiti.¬†</span></p><h2>Come funziona un LLM: dall‚Äôinput alla risposta</h2><p><span style="font-weight: 400;">Immaginiamo di chiedere a un modello come ChatGPT qualcosa di creativo. Ad esempio: </span><i><span style="font-weight: 400;">‚ÄúC‚Äôera una volta un regno incantato dove viveva un giovane drago. Un giorno‚Ä¶‚Äù</span></i><span style="font-weight: 400;"> e gli domandiamo di continuare la storia. In pochi secondi l‚ÄôLLM produce magari un racconto avvincente, pieno di avventure. </span><strong>Cosa √® successo, in termini semplici, dentro il ‚Äúcervello‚Äù artificiale del modello?</strong><span style="font-weight: 400;"> Ecco le fasi principali del processo:</span></p><ol><li style="font-weight: 400;" aria-level="1"><strong>Comprensione del prompt (input):</strong><span style="font-weight: 400;"> il testo della nostra richiesta viene prima </span><i><span style="font-weight: 400;">preparato</span></i><span style="font-weight: 400;"> in modo che il modello possa lavorarci. In pratica l‚ÄôLLM spezzetta la frase in unit√† pi√π piccole chiamate </span><strong>token</strong><span style="font-weight: 400;"> (parole o frammenti di parole) e le converte in numeri. √à un po‚Äô come tradurre la frase in un linguaggio matematico che la macchina possa elaborare.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Analisi del contesto:</strong><span style="font-weight: 400;"> a questo punto i token numerici vengono passati attraverso la rete neurale del modello. L‚ÄôLLM </span><i><span style="font-weight: 400;">esamina il contesto</span></i><span style="font-weight: 400;"> della frase e, sulla base di ci√≤ che ‚Äúha imparato‚Äù da miliardi di parole di training, stima quale potrebbe essere la parola (o token) pi√π probabile dopo quelle gi√† date. In questa fase interviene il </span><strong>meccanismo di attenzione</strong><span style="font-weight: 400;"> tipico dei trasformatori: il modello valuta in parallelo tutti i termini presenti nel prompt e assegna pi√π peso a quelli pi√π rilevanti, ignorando quelli meno significativi. In altre parole, ‚Äúcapisce‚Äù quali concetti chiave sono emersi finora (es. </span><i><span style="font-weight: 400;">regno incantato</span></i><span style="font-weight: 400;">, </span><i><span style="font-weight: 400;">giovane drago</span></i><span style="font-weight: 400;">) e li usa per predire come la storia potrebbe proseguire.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Previsione della prossima parola:</strong><span style="font-weight: 400;"> in base all‚Äôanalisi, l‚ÄôLLM produce una sorta di elenco di possibili continuazioni, ciascuna con una certa </span><strong>probabilit√† statistica</strong><span style="font-weight: 400;">. Ad esempio, dopo la frase </span><i><span style="font-weight: 400;">‚ÄúC‚Äôera una volta un giovane drago‚Äù</span></i><span style="font-weight: 400;"> il modello potrebbe calcolare che il token </span><i><span style="font-weight: 400;">‚Äúche‚Äù</span></i><span style="font-weight: 400;"> ha il 40% di probabilit√† di venire dopo, </span><i><span style="font-weight: 400;">‚Äúdragone‚Äù</span></i><span style="font-weight: 400;"> il 15%, </span><i><span style="font-weight: 400;">‚Äúdi‚Äù</span></i><span style="font-weight: 400;"> il 10%, </span><i><span style="font-weight: 400;">‚Äúprincipe‚Äù</span></i><span style="font-weight: 400;"> il 5%, e cos√¨ via ‚Äì a seconda di ci√≤ che ha appreso dai testi durante l‚Äôaddestramento. In effetti, l‚ÄôLLM non inventa dal nulla la continuazione: sta scegliendo la parola successiva basandosi sui </span><strong>pattern linguistici</strong><span style="font-weight: 400;"> che ha gi√† visto moltissime volte.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Generazione iterativa della risposta:</strong><span style="font-weight: 400;"> una volta ottenute le probabilit√†, il modello </span><strong>sceglie un token</strong><span style="font-weight: 400;"> come prossimo elemento della risposta. Spesso seleziona quello con la probabilit√† pi√π alta, ma pu√≤ introdurre un po‚Äô di casualit√† per rendere il testo meno prevedibile. Immaginiamo che scelga ad esempio </span><i><span style="font-weight: 400;">‚Äúche‚Äù</span></i><span style="font-weight: 400;">. A questo punto il token </span><i><span style="font-weight: 400;">‚Äúche‚Äù</span></i><span style="font-weight: 400;"> viene aggiunto alla frase generata e il modello </span><strong>ripete</strong><span style="font-weight: 400;"> nuovamente il passo 2: rianalizza tutto il contesto aggiornato (</span><i><span style="font-weight: 400;">‚ÄúC‚Äôera una volta un giovane drago che‚Äù</span></i><span style="font-weight: 400;">) e calcola il token successivo. Poi di nuovo e cos√¨ via, parola dopo parola, </span><strong>fino a completare la frase o il paragrafo</strong><span style="font-weight: 400;"> richiesto. Questo processo a catena continua finch√© l‚ÄôLLM produce un segnale di </span><strong>fine risposta</strong><span style="font-weight: 400;"> (ad esempio un token speciale di stop) oppure finch√© raggiunge un limite di lunghezza prestabilito.</span><span style="font-weight: 400;"><br><br></span></li></ol><p><span style="font-weight: 400;">Possiamo paragonare il tutto a </span><strong>un super-autocompletamento</strong><span style="font-weight: 400;"> intelligente: come quando il telefono ci suggerisce le parole mentre digitiamo un messaggio, ma in questo caso con una potenza e una base di conoscenza immensamente pi√π grandi. L‚ÄôLLM, addestrato su innumerevoli esempi di frasi, </span><i><span style="font-weight: 400;">prevede</span></i><span style="font-weight: 400;"> di volta in volta il pezzo mancante successivo, cucendo insieme una risposta frase dopo frase.</span></p><blockquote><h5>üîç Token ed embedding: l‚Äôalfabeto segreto</h5><p><strong><i><br></i></strong><span style="font-weight: 400;">Un computer non ‚Äúcomprende‚Äù realmente le parole testuali: deve rappresentarle con dei numeri. La </span><strong>tokenizzazione</strong><span style="font-weight: 400;"> √® il procedimento che converte il testo in piccoli segmenti (token) che possono essere elaborati dal modello. Spesso i token corrispondono a parole intere, ma possono anche essere sillabe o addirittura singole lettere nei casi complessi. Ad ogni token l‚ÄôLLM associa poi un </span><strong>embedding</strong><span style="font-weight: 400;">, ovvero un </span><strong>vettore di numeri</strong><span style="font-weight: 400;"> (di solito decine o centinaia di valori) che rappresenta quella parola in forma matematica all‚Äôinterno dello spazio ‚Äúmentale‚Äù del modello. L‚Äôidea √® che parole con significato o uso simile avranno vettori (embedding) </span><i><span style="font-weight: 400;">simili</span></i><span style="font-weight: 400;"> tra loro. Ad esempio, in inglese </span><i><span style="font-weight: 400;">‚Äúsea‚Äù</span></i><span style="font-weight: 400;"> e </span><i><span style="font-weight: 400;">‚Äúocean‚Äù</span></i><span style="font-weight: 400;"> (cio√® </span><i><span style="font-weight: 400;">mare</span></i><span style="font-weight: 400;"> e </span><i><span style="font-weight: 400;">oceano</span></i><span style="font-weight: 400;">) risultano molto vicini (perch√© simili) nello spazio vettoriale degli embedding ‚Äì segno che il modello li considera concetti affini. In pratica, l‚Äôembedding √® come un‚Äôimpronta numerica che cattura il senso di un token: √® grazie a queste rappresentazioni che l‚ÄôLLM pu√≤ ‚Äúragionare‚Äù sulle parole in ingresso e trovare connessioni tra termini correlati.</span></p></blockquote><h2>Dentro la scatola nera: com‚Äô√® fatto (e addestrato) un LLM</h2><p><span style="font-weight: 400;">Abbiamo visto a grandi linee </span><i><span style="font-weight: 400;">come</span></i><span style="font-weight: 400;"> un LLM genera testo. Ma com‚Äô√® strutturato internamente questo ‚Äúscrittore automatico‚Äù? La risposta breve: √® una </span><strong>rete neurale</strong><span style="font-weight: 400;"> enorme, con miliardi di connessioni, addestrata su </span><strong>quantit√† mastodontiche di dati testuali</strong><span style="font-weight: 400;">. Due ingredienti, </span><i><span style="font-weight: 400;">scala</span></i><span style="font-weight: 400;"> e </span><i><span style="font-weight: 400;">quantit√†</span></i><span style="font-weight: 400;">, sono stati fondamentali per il salto di qualit√† di questi modelli negli ultimi anni.</span></p><ul><li style="font-weight: 400;" aria-level="1"><strong>Dati in abbondanza (token):</strong><span style="font-weight: 400;"> i Large Language Model vengono </span><i><span style="font-weight: 400;">pre-addestrati</span></i><span style="font-weight: 400;"> leggendo praticamente tutto quello che trovano. Documenti pubblici, libri, articoli, pagine web, conversazioni di forum ‚Äì un vero tesoro linguistico. Il modello ‚Äúdigerisce‚Äù questo corpus immenso imparando, tramite un lungo processo di ottimizzazione, a predire la parola successiva in ogni frase. Ad esempio, il modello </span>GPT-3<span style="font-weight: 400;"> di OpenAI (sviluppato nel 2020) √® stato addestrato usando circa </span>300 miliardi di token, che corrispondono a circa 45‚Äì60 TB di dati testuali<strong>¬†</strong>(sui modelli successivi non si hanno informazioni ma si ipotizza tra 1 e 5 trilioni di token)<span style="font-weight: 400;">. Durante il training, l‚ÄôLLM macina questi testi miliardi di volte, aggiustando gradualmente i propri parametri per migliorare la capacit√† di indovinare le parole seguenti. </span>Un token pu√≤ essere una parola intera, una radice (es. <em data-start="472" data-end="479">scriv</em>), una sillaba, o persino una singola lettera o segno di punteggiatura. Tutto dipende dal metodo di tokenizzazione scelto. La frase ‚Äú<em>Ciao, come stai?</em>‚Äù potrebbe diventare semplicisticamente¬† ‚Üí <code data-start="666" data-end="702">["Ciao", ",", "come", "stai", "?"]</code> ‚Üí <strong data-start="705" data-end="716">5 token</strong>. Ma i metodi sono i pi√π disparati. La stessa frase ChatGPT la "<em><a href="https://platform.openai.com/tokenizer">tokenizza</a></em>" ancora pi√π finemente:<figure class="post__image"><img loading="lazy" src="https://pianetararo.org/media/posts/22/token-Screenshot-2025-06-11-121527.png" alt="" width="398" height="367" sizes="(max-width: 1920px) 100vw, 1920px" srcset="https://pianetararo.org/media/posts/22/responsive/token-Screenshot-2025-06-11-121527-xs.png 640w, https://pianetararo.org/media/posts/22/responsive/token-Screenshot-2025-06-11-121527-sm.png 768w, https://pianetararo.org/media/posts/22/responsive/token-Screenshot-2025-06-11-121527-md.png 1024w, https://pianetararo.org/media/posts/22/responsive/token-Screenshot-2025-06-11-121527-lg.png 1366w, https://pianetararo.org/media/posts/22/responsive/token-Screenshot-2025-06-11-121527-xl.png 1600w, https://pianetararo.org/media/posts/22/responsive/token-Screenshot-2025-06-11-121527-2xl.png 1920w"></figure>¬†</li></ul><p>¬†</p><ul><li style="font-weight: 400;" aria-level="1"><strong>Tanti parametri (il ‚Äúcervello‚Äù della rete):</strong><span style="font-weight: 400;"> se i token sono i pezzi di linguaggio, i parametri sono le <strong data-start="1145" data-end="1168">connessioni apprese</strong> durante l‚Äôaddestramento. I parametri di un modello sono i coefficienti numerici (pesi) che collegano i neuroni artificiali al suo interno. Ogni parametro √® come una manopola o un interruttore che il modello pu√≤ regolare mentre impara. Quando il modello √® addestrato, queste ‚Äúmanopole‚Äù restano fisse e determinano come l‚ÄôIA risponde. Pi√π parametri ci sono, pi√π </span><i><span style="font-weight: 400;">capacit√† espressiva</span></i><span style="font-weight: 400;"> ha la rete neurale ‚Äì in teoria ‚Äì per cogliere sfumature e pattern del linguaggio. I moderni LLM sono giganteschi: GPT-3, ad esempio, ha </span><strong>175 miliardi di parametri</strong><span style="font-weight: 400;">. Per dare un‚Äôidea, il suo predecessore GPT-2 ne aveva ‚Äúsolo‚Äù 1,5 miliardi. Questa crescita esponenziale ha permesso al modello di </span><strong>immagazzinare conoscenza linguistica</strong><span style="font-weight: 400;"> molto pi√π approfondita (anche se non organizzata come una conoscenza umana, ovviamente). Pi√π neuroni e connessioni significano che il modello pu√≤ rappresentare concetti molto complessi e variegati. Naturalmente addestrare modelli del genere richiede risorse computazionali enormi (supercomputer con migliaia di GPU che macinano dati per settimane), ma il risultato √® un‚ÄôIA capace di generare testi straordinariamente coerenti.</span></li></ul><p data-start="1909" data-end="1965">Immaginiamo che un LLM sia uno <strong data-start="1937" data-end="1964">chef che scrive ricette</strong>:</p><ul data-start="1967" data-end="2199"><li data-start="1967" data-end="2049"><p data-start="1969" data-end="2049">I <strong data-start="1971" data-end="1980">token</strong> sono gli <strong data-start="1990" data-end="2005">ingredienti</strong> che lo chef usa (le parole da combinare).</p></li><li data-start="2050" data-end="2199"><p data-start="2052" data-end="2199">I <strong data-start="2054" data-end="2067">parametri</strong> sono le <strong data-start="2076" data-end="2101">esperienze accumulate</strong>, le prove e gli errori che gli hanno insegnato a cucinare bene (cio√® a generare testo sensato).</p></li></ul><p data-start="2201" data-end="2300">Pi√π parametri = uno chef pi√π esperto.<br data-start="2238" data-end="2241">Pi√π token = pi√π ingredienti con cui preparare nuovi piatti.</p><p><span style="font-weight: 400;">A dare veramente il via a questa nuova generazione di IA linguistiche √® stata per√≤ una </span><strong>svolta architetturale</strong><span style="font-weight: 400;">. Fino a qualche anno fa, i modelli di elaborazione del linguaggio (come i traduttori automatici o i vecchi chatbot) si basavano in gran parte su reti neurali ricorrenti o sequenziali, che leggevano il testo parola per parola in ordine. Nel </span><strong>2017</strong><span style="font-weight: 400;">, un team di ricercatori di Google ha introdotto un modello diverso, chiamato </span><strong>Transformer</strong><span style="font-weight: 400;"> (trasformatore), descrivendolo in un celebre paper scientifico dal titolo </span><i><span style="font-weight: 400;">‚ÄúAttention Is All You Need‚Äù</span></i><span style="font-weight: 400;">. Questa architettura ha rivoluzionato il campo perch√© </span><strong>analizza una sequenza di testo in parallelo anzich√© in sequenza</strong><span style="font-weight: 400;">, usando un meccanismo di </span><strong>self-attention</strong><span style="font-weight: 400;"> (auto-attenzione) che permette al modello di </span><i><span style="font-weight: 400;">concentrarsi</span></i><span style="font-weight: 400;"> sui punti importanti di una frase ignorandone i dettagli meno rilevanti. In pratica il trasformatore guarda all‚Äôintera frase (o anche interi paragrafi) tutta in una volta, e per ogni parola capisce quali altre parole del contesto sono pi√π utili per interpretarla. √à un po‚Äô quello che facciamo noi leggendo: se diciamo ‚Äú</span><strong>il giovane drago</strong><span style="font-weight: 400;"> impar√≤ a volare‚Äù, il nostro cervello collega </span><i><span style="font-weight: 400;">drago</span></i><span style="font-weight: 400;"> con </span><i><span style="font-weight: 400;">volare</span></i><span style="font-weight: 400;"> e non d√† peso a parole come </span><i><span style="font-weight: 400;">il</span></i><span style="font-weight: 400;"> o </span><i><span style="font-weight: 400;">a</span></i><span style="font-weight: 400;">. Il </span><strong>Transformer</strong><span style="font-weight: 400;"> permette all‚ÄôLLM di fare lo stesso tipo di collegamenti in maniera efficiente e accurata.</span></p><p><span style="font-weight: 400;">Questa innovazione ha reso possibili modelli con contesti molto ampi (capaci di ‚Äúricordare‚Äù e tenere in considerazione molte frasi precedenti) e con output di qualit√† molto pi√π alta. Non sorprende che tutti i principali LLM odierni ‚Äì da GPT-3 e GPT-4 di OpenAI ai modelli di Google, Meta, Anthropic ecc. ‚Äì siano basati su architettura transformer. Per chi volesse vedere con i propri occhi come funziona un trasformatore, il </span><strong>Financial Times</strong><span style="font-weight: 400;"> ha pubblicato un bellissimo </span><strong>articolo interattivo</strong><span style="font-weight: 400;"> che illustra passo passo questi meccanismi. Scorrendo quella pagina web speciale, √® possibile visualizzare come un LLM codifica le parole, come l‚Äôattenzione evidenzia certe parole chiave, e persino come possono emergere fenomeni come le </span><i><span style="font-weight: 400;">allucinazioni</span></i><span style="font-weight: 400;"> durante la generazione di testo. √à un ottimo complemento visuale a quanto stiamo raccontando qui.</span></p><h2>LLM ‚â† intelligenza umana: miti e realt√†</h2><p><span style="font-weight: 400;">Dopo tutto quello che abbiamo visto, potrebbe sorgere spontanea una domanda: </span><i><span style="font-weight: 400;">ma dunque questi LLM sono davvero ‚Äúintelligenti‚Äù?</span></i><span style="font-weight: 400;"> Dipende da cosa intendiamo per intelligente. Certo, </span><strong>sanno generare testo di senso compiuto</strong><span style="font-weight: 400;">, usare un lessico ricco, perfino imitare stili letterari. Tuttavia, </span><strong>non possiedono una comprensione profonda</strong><span style="font-weight: 400;"> di ci√≤ che scrivono, n√© coscienza o intenzionalit√†. Spesso attribuiamo loro caratteristiche umane (c‚Äô√® chi chiede al chatbot se </span><i><span style="font-weight: 400;">prova emozioni</span></i><span style="font-weight: 400;"> o se </span><i><span style="font-weight: 400;">pensa come noi</span></i><span style="font-weight: 400;">), ma √® un‚Äôillusione. In realt√† un LLM </span><strong>non ‚Äúcapisce‚Äù il significato</strong><span style="font-weight: 400;"> come lo capiremmo noi, ma manipola simboli statistici. Alcuni ricercatori hanno icasticamente definito questi modelli </span><i><span style="font-weight: 400;">‚Äúpappagalli stocastici‚Äù</span></i><span style="font-weight: 400;">: in pratica, imitano il linguaggio umano senza averne la </span><i><span style="font-weight: 400;">consapevolezza</span></i><span style="font-weight: 400;">.</span></p><p><span style="font-weight: 400;">Conoscere la struttura e il metodo di generazione di un LLM ci aiuta a </span><strong>ridimensionare alcune aspettative</strong><span style="font-weight: 400;"> e ad usarlo con maggiore senso critico. Ecco alcuni punti da tenere a mente quando interagiamo con queste IA:</span></p><ul><li style="font-weight: 400;" aria-level="1"><strong>Non hanno vera comprensione o conoscenza</strong><span style="font-weight: 400;"> ‚Äì Un LLM non </span><i><span style="font-weight: 400;">sa</span></i><span style="font-weight: 400;"> di cosa parla; mette insieme frasi basate sulle probabilit√† apprese. Pu√≤ scrivere </span><i><span style="font-weight: 400;">‚ÄúIl Sole √® una stella‚Äù</span></i><span style="font-weight: 400;"> perch√© ha visto spesso quella frase, ma non ha in testa un modello del sistema solare. Come detto, √® un </span><i><span style="font-weight: 400;">pappagallo statistico</span></i><span style="font-weight: 400;"> che ripete strutture linguistiche plausibili senza ancorarle a un significato vero. Dunque, anche se le sue risposte </span><strong>suonano</strong><span style="font-weight: 400;"> competenti, l‚ÄôLLM non dispone di un vero </span><strong>filtro di verit√†</strong><span style="font-weight: 400;"> o di logica: sta solo seguendo i pattern appresi.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Possono </strong><strong><i>allucinare</i></strong><strong> (inventare) fatti</strong><span style="font-weight: 400;"> ‚Äì Capita spesso che un LLM fornisca informazioni del tutto sbagliate con tono sicuro. Ad esempio, potrebbe affermare una data storica errata, citare una legge inesistente o mischiare biografie di persone diverse. Questo avviene perch√© il modello </span><strong>non ha modo di verificare</strong><span style="font-weight: 400;"> le sue affermazioni: se certi termini compaiono frequentemente insieme nei suoi dati di training, lui li user√†, anche se l‚Äôinformazione √® falsa. In gergo si parla di </span><i><span style="font-weight: 400;">allucinazioni dell‚ÄôIA</span></i><span style="font-weight: 400;">. Il problema √® che l‚ÄôLLM </span><strong>non sa di non sapere</strong><span style="font-weight: 400;">: non ha un feedback interno che gli dica ‚Äúquesta cosa √® sbagliata‚Äù. Un gruppo di esperti ha sottolineato proprio che, </span><i><span style="font-weight: 400;">‚Äúsiccome si limita a mettere insieme parole in base ai dati di addestramento, un LLM non si rende conto se sta dicendo qualcosa di scorretto o inopportuno‚Äù</span></i><span style="font-weight: 400;">. Sta a noi utenti, quindi, fare da filtro: </span><strong>mai prendere per oro colato</strong><span style="font-weight: 400;"> tutto ci√≤ che esce da ChatGPT &amp; co., specialmente dati di fatto importanti, senza una verifica esterna.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Hanno i bias dei dati (e dei programmatori)</strong><span style="font-weight: 400;"> ‚Äì Un LLM assorbe pregiudizi, errori e punti di vista presenti nei testi con cui √® stato addestrato. Se gran parte del web contiene un bias (ad esempio stereotipi di genere o discriminazioni razziali), il modello pu√≤ rifletterli nelle sue risposte. E anche se ci sono processi di correzione e filtraggio, non esiste garanzia assoluta di neutralit√† o accuratezza. In pi√π, i </span><strong>valori</strong><span style="font-weight: 400;"> che guidano il modello (cosa considera accettabile o meno dire) dipendono in larga misura da scelte dei suoi sviluppatori durante la fase di fine-tuning. Questo non significa che l‚ÄôLLM sia ‚Äúmalvagio‚Äù o manipolatore di per s√© ‚Äì ricordiamo, non ha volont√† ‚Äì ma che </span><strong>pu√≤ ereditare sia il meglio sia il peggio</strong><span style="font-weight: 400;"> del materiale su cui √® stato addestrato.</span><span style="font-weight: 400;"><br><br></span></li><li style="font-weight: 400;" aria-level="1"><strong>Non provano emozioni (anche se le simulano)</strong><span style="font-weight: 400;"> ‚Äì Gli LLM possono scrivere frasi molto empatiche (</span><i><span style="font-weight: 400;">‚ÄúMi dispiace che tu stia attraversando questo momento difficile‚Ä¶‚Äù</span></i><span style="font-weight: 400;">), ma ci√≤ non implica alcuna esperienza emotiva da parte loro. Quando leggiamo una risposta accorata di un chatbot, siamo tentati di attribuirle un‚Äôintenzione o una sensibilit√†; in realt√† il modello sta semplicemente riproducendo schemi linguistici tipici delle conversazioni empatiche che ha visto nei suoi dati. </span><strong>Non c‚Äô√® un ‚Äúio‚Äù dietro quelle parole</strong><span style="font-weight: 400;">, nessuna coscienza che soffre, gode o tiene davvero a noi. √à fondamentale ricordarlo per evitare di prendere decisioni emotive basate su una falsa percezione dell‚ÄôIA (ad esempio, sentirsi giudicati da lei, o credere che ‚Äúci tenga‚Äù davvero). L‚ÄôLLM √® </span><i><span style="font-weight: 400;">abile imitazione</span></i><span style="font-weight: 400;"> di un parlante umano, ma resta un‚Äôimitazione.</span><span style="font-weight: 400;"><br><br></span></li></ul><p><span style="font-weight: 400;">Conoscere </span><i><span style="font-weight: 400;">come √® fatto e come lavora</span></i><span style="font-weight: 400;"> un Large Language Model ci aiuta a usarlo in modo pi√π </span><strong>maturo e consapevole</strong><span style="font-weight: 400;">. Possiamo ammirarne le capacit√† ‚Äì perch√© √® straordinario che un software </span><strong>autocompleti</strong><span style="font-weight: 400;"> testi cos√¨ bene da sembrare creativi o competenti ‚Äì </span><strong>senza per√≤ mitizzarlo</strong><span style="font-weight: 400;">. Un LLM non √® un oracolo infallibile n√© una mente artificiale dotata di saggezza propria: √® un potente strumento statistico, un prodotto dell‚Äôingegno umano (ricerca, algoritmi e tonnellate di dati). Sta a noi utilizzarlo come </span><i><span style="font-weight: 400;">amplificatore</span></i><span style="font-weight: 400;"> della nostra creativit√† e produttivit√†, ma anche come soggetto da monitorare. In fondo, </span><strong>dietro le quinte di un‚ÄôIA linguistica non c‚Äô√® magia</strong><span style="font-weight: 400;"> ‚Äì c‚Äô√® matematica, informatica e tanta probabilit√†. E pi√π ne siamo consapevoli, meglio potremo sfruttare queste nuove tecnologie senza farci sfruttare da esse.</span></p><h2>Da dove arrivano i dati? (Spoiler: anche da te)¬†</h2><p><span style="font-weight: 400;">Immagina per un attimo un‚ÄôIA come un grande scrittore che non ha mai vissuto nulla in prima persona. Non ha viaggiato, non ha amato, non ha mai mangiato una pizza vera. Eppure scrive poesie, articoli, dialoghi brillanti. Come fa? Semplice: </span><strong><i>legge tutto quello che trova</i></strong><span style="font-weight: 400;">. Interi oceani di parole ‚Äì siti web, manuali, romanzi, ricette, forum, tweet, commenti, newsletter, persino battute da meme e recensioni su Amazon. Se c‚Äô√® stato un momento in cui hai pubblicato qualcosa online, c‚Äô√® una buona possibilit√† che l‚ÄôIA lo abbia letto (o almeno scannerizzato) in silenzio.</span></p><p><span style="font-weight: 400;">Ma ecco il punto critico: per anni si √® attinto a piene mani da Internet ‚Äì tutto a portata di clic. Il problema √® che molti di quei contenuti sono protetti da copyright: libri, articoli, blog, canzoni, manuali. E non sempre chi li ha scritti ha dato il permesso di usarli. Negli Stati Uniti, diversi autori, editori e giornalisti hanno fatto causa a colossi come OpenAI e Meta, accusandoli di aver "ingurgitato" opere intere senza autorizzazione per addestrare i loro modelli. Le aziende, dal canto loro, si difendono invocando il ‚Äúfair use‚Äù, una clausola del diritto americano che permette l‚Äôuso di opere protette in certi casi (come ricerca o parodia). Ma il confine tra uso lecito e sfruttamento improprio √® sempre pi√π sottile, specialmente se l‚Äôoutput dell‚ÄôIA finisce per somigliare troppo a un contenuto originale. In Europa, intanto, si spingono nuove regole per obbligare le aziende a dichiarare chiaramente </span><strong>che dati usano, dove li prendono e per che cosa</strong><span style="font-weight: 400;">. Insomma, si sta cercando di trasformare la scatola nera dei LLM in una finestra (almeno socchiusa) sulla loro memoria. E non √® una battaglia da poco: in gioco non c‚Äô√® solo la propriet√† intellettuale, ma il diritto delle persone a sapere se ‚Äì e come ‚Äì i loro contenuti vengono usati per creare ‚Äúl‚Äôintelligenza‚Äù delle macchine.</span></p><p><span style="font-weight: 400;">Ultimamente, parlare di come vengono ‚Äúallenate‚Äù le intelligenze artificiali significa entrare in una cucina in pieno fermento. Gli ingredienti sono testi presi dal web, grandi banche dati, righe di codice, dati artificiali creati da altre AI, e ‚Äì sempre pi√π spesso ‚Äì un pizzico di controversia legale. Le grandi aziende del settore lavorano costantemente per rendere questi modelli pi√π performanti, pi√π affidabili, pi√π ‚Äúumani‚Äù nel modo di rispondere. Ma non si tratta solo di potenza di calcolo: il vero salto lo stanno facendo con tecniche di fine-tuning mirato e dati sintetici. Detto in modo semplice, si prendono modelli generici e li si ‚Äúriprogramma‚Äù su settori specifici ‚Äì medicina, giurisprudenza, creativit√† ‚Äì usando dati nuovi, a volte prodotti proprio da un‚Äôaltra IA. Un po‚Äô come allenare un buon musicista a specializzarsi nel jazz, dopo anni di classica. Si aggiungono e affiancano anche altre tecniche prima di arrivare alla risposta vera e propria come ad esempio il Reinforcement Learning from Human Feedback: s√¨, l‚ÄôAI viene corretta anche a mano da persone in carne e ossa, che le dicono cosa suona naturale, cosa √® fuori luogo, cosa √® utile (avete presente quel pollice su/gi√π che compare alla fine in ogni risposta?). E questo, strano a dirsi, √® ci√≤ che la rende un po‚Äô meno ‚Äúmacchina‚Äù e un po‚Äô pi√π ‚Äúdialogante‚Äù.</span></p><p>¬†</p><h2>LLM con le mani: la nuova generazione di AI che pensa e agisce</h2><p><span style="font-weight: 400;">L‚ÄôAI oggi non si accontenta pi√π di rispondere a una domanda: vuole (o meglio, pu√≤) agire. Stiamo assistendo a un passaggio cruciale, quello dagli LLM tradizionali ‚Äì che producono qualcosa e si fermano l√¨ ‚Äì a </span><strong>sistemi agentici</strong><span style="font-weight: 400;">, cio√® agenti digitali capaci di compiere azioni complesse, in autonomia, su pi√π passaggi. Sembra fantascienza, ma non lo √®. Immagina di chiedere a un‚ÄôIA: ‚Äú</span><i><span style="font-weight: 400;">Prenotami un viaggio per Roma a ottobre, con hotel vicino al centro e voli in orari comodi.</span></i><span style="font-weight: 400;">‚Äù Fino a poco fa avresti ricevuto un elenco di opzioni. Oggi, un </span><i><span style="font-weight: 400;">agente AI</span></i><span style="font-weight: 400;"> come quelli su cui stanno lavorando OpenAI, Google o Meta pu√≤ effettivamente aprire siti, cercare voli e hotel, confrontare prezzi, compilare moduli, e ‚Äì in certi casi ‚Äì completare il tutto. Pu√≤ interagire direttamente e in modo autonomo con piattaforme e servizi di terze parti. Il tutto seguendo una logica propria, imparata in fase di addestramento, pre-allenata dal fornitore dello strumento e </span><i><span style="font-weight: 400;">senza che tu debba intervenire di continuo</span></i><span style="font-weight: 400;">. E non parliamo pi√π solo di esperimenti in laboratorio: OpenAI, ad esempio, ha presentato ‚ÄúOperator‚Äù, un prototipo capace di usare un computer virtuale come farebbe un utente umano, con tanto di mouse e tastiera simulati. √à stato addestrato non solo a generare testo, ma a </span><i><span style="font-weight: 400;">capire cosa fare</span></i><span style="font-weight: 400;"> in contesti digitali complessi, adattandosi passo dopo passo. La novit√†? Questi agenti non sono pi√π solo bravi a scrivere, produrre immagini, audio o video¬† ‚Äì stanno diventando</span><strong> </strong><span style="font-weight: 400;">capaci di </span><strong>comprendere il contesto</strong><span style="font-weight: 400;"> e¬† </span><strong>portare a termine </strong><strong><i>task</i></strong><strong> pratici</strong><span style="font-weight: 400;">. E, se tutto funziona come sperano le big tech, potremmo presto avere agenti personali evoluti che ci gestiscono email, documenti, viaggi e appuntamenti o interi flussi di attivit√† complesse.¬†</span></p><p><span style="font-weight: 400;">Ma, ovviamente, qui iniziano anche le </span><strong>domande spinose, </strong><span style="font-weight: 400;">per ora ce ne poniamo solo qualcuna dato che la lista potrebbe essere molto pi√π ricca:¬†</span></p><ul><li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Quanto possiamo fidarci di un software che agisce al posto nostro?¬†</span></li><li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Abbiamo davvero controllo su ci√≤ che l‚Äôagente fa quando agisce ‚Äúda solo‚Äù?¬†</span></li><li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">E come assicurarci che faccia davvero ci√≤ che vogliamo ‚Äì e non altro?¬†</span></li><li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Chi √® responsabile se un agente AI prende una decisione sbagliata o dannosa?¬†</span></li><li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Chi controlla i filtri, i limiti e le ‚Äúzone cieche‚Äù dell‚Äôagente AI?¬†</span></li><li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Quanto √® accettabile ‚Äì eticamente e culturalmente ‚Äì permettere che un software ‚Äúparli e agisca per noi‚Äù?¬†</span></li><li style="font-weight: 400;" aria-level="1"><span style="font-weight: 400;">Soprattutto nei contesti educativi, professionali o affettivi: vogliamo che le nostre parole siano le nostre, o solo le pi√π efficienti?</span></li></ul><hr><p>Questo post √® parte della rubrica <strong><a href="https://pianetararo.org/traiettorie/">TrAIettorie</a></strong> di cui potete trovare l'indice completo <a href="https://pianetararo.org/tags/traiettorie/">qui</a>.</p></div><footer class="content__footer"><div class="entry-wrapper"><div class="content__actions"><ul class="content__tag"><li><a href="https://pianetararo.org/tags/traiettorie/">TRAIETTORIE</a></li></ul><div class="content__share"><button class="btn--icon content__share-button js-content__share-button"><svg width="20" height="20" aria-hidden="true"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#share"></use></svg> <span>Share It</span></button><div class="content__share-popup js-content__share-popup"><a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fpianetararo.org%2Fcome-funzionano-le-ia-che-ci-scrivono%2F" class="js-share facebook" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#facebook"/></svg> <span>Facebook</span> </a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fpianetararo.org%2Fcome-funzionano-le-ia-che-ci-scrivono%2F" class="js-share linkedin" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#linkedin"/></svg> <span>LinkedIn</span> </a><a href="https://api.whatsapp.com/send?text=Come%20funzionano%20le%20AI%20che%20ci%20scrivono https%3A%2F%2Fpianetararo.org%2Fcome-funzionano-le-ia-che-ci-scrivono%2F" class="js-share whatsapp" rel="nofollow noopener noreferrer"><svg class="icon" aria-hidden="true" focusable="false"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#whatsapp"/></svg> <span>WhatsApp</span></a></div></div></div></div></footer></article></main><footer class="footer"><div class="wrapper"><div class="footer__copyright"><div class="block-footer-fl" style="width: 100%; height: 100%; , max-width: 100%;"><p class="align-center" style="font-size: x-small;"><span style="color: #44684b;">Pianetararo associazione culturale</span><br><span style="color: #44684b;">CF: 04015870365</span><br><span style="color: #44684b;">info@pianetararo.org</span><br><span style="color: #44684b;">Pianetararo √® un associazione senza scopo di lucro e partecipa al programma "Google for Non profits".</span></p><p class="align-center" style="font-size: x-small;"><span style="color: #44684b;"><a href="https://pianetararo.org/privacy-and-cookie/" title="Privacy &amp; Cookie policy" style="color: #44684b;">Privacy &amp; Cookie policy</a></span></p></div></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg width="20" height="20"><use xlink:href="https://pianetararo.org/assets/svg/svg-map.svg#toparrow"/></svg></button></div></footer><script defer="defer" src="https://pianetararo.org/assets/js/scripts.min.js?v=700105c316933a8202041b6415abb233"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script><div class="pcb" data-behaviour="badge" data-behaviour-link="#cookie-settings" data-revision="1" data-config-ttl="90" data-debug-mode="false"><div role="dialog" aria-modal="true" aria-hidden="true" aria-labelledby="pcb-title" aria-describedby="pcb-txt" class="pcb__banner"><div class="pcb__inner"><div id="pcb-title" role="heading" aria-level="2" class="pcb__title">This website uses cookies</div><div id="pcb-txt" class="pcb__txt">Select which cookies to opt-in to via the checkboxes below; our website uses cookies to examine site traffic and user activity while on our site, for marketing, and to provide social media functionality. <a href="https://pianetararo.org/privacy-and-cookie/">More details...</a></div><div class="pcb__buttons"><button type="button" class="pcb__btn pcb__btn--solid pcb__btn--accept">Accept all</button></div></div></div><button class="pcb__badge" aria-label="Cookie Policy" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" width="40" height="40" viewBox="0 0 23 23" fill="currentColor"><path d="M21.41 12.71c-.08-.01-.15 0-.22 0h-.03c-.03 0-.05 0-.08.01-.07 0-.13.01-.19.04-.52.21-1.44.19-2.02-.22-.44-.31-.65-.83-.62-1.53a.758.758 0 0 0-.27-.61.73.73 0 0 0-.65-.14c-1.98.51-3.49.23-4.26-.78-.82-1.08-.73-2.89.24-4.49.14-.23.14-.52 0-.75a.756.756 0 0 0-.67-.36c-.64.03-1.11-.1-1.31-.35-.19-.26-.13-.71-.01-1.29.04-.18.06-.38.03-.59-.05-.4-.4-.7-.81-.66C5.1 1.54 1 6.04 1 11.48 1 17.28 5.75 22 11.6 22c5.02 0 9.39-3.54 10.39-8.42.08-.4-.18-.78-.58-.87Zm-9.81 7.82c-5.03 0-9.12-4.06-9.12-9.06 0-4.34 3.05-8 7.25-8.86-.08.7.05 1.33.42 1.81.24.32.66.67 1.38.84-.76 1.86-.65 3.78.36 5.11.61.81 2.03 2 4.95 1.51.18.96.71 1.54 1.18 1.87.62.43 1.38.62 2.1.62.05 0 .09 0 .13-.01-1.23 3.64-4.7 6.18-8.64 6.18ZM13 17c0 .55-.45 1-1 1s-1-.45-1-1 .45-1 1-1 1 .45 1 1Zm5.29-12.3a.99.99 0 0 1-.29-.71c0-.55.45-.99 1-.99a1 1 0 0 1 .71.3c.19.19.29.44.29.71 0 .55-.45.99-1 .99a1 1 0 0 1-.71-.3ZM9 13.5c0 .83-.67 1.5-1.5 1.5S6 14.33 6 13.5 6.67 12 7.5 12s1.5.67 1.5 1.5Zm3.25.81a.744.744 0 0 1-.06-1.05c.28-.32.75-.34 1.05-.06.31.28.33.75.05 1.06-.15.16-.35.25-.56.25-.18 0-.36-.06-.5-.19ZM8.68 7.26c.41.37.44 1 .07 1.41-.2.22-.47.33-.75.33a.96.96 0 0 1-.67-.26c-.41-.37-.44-1-.07-1.41.37-.42 1-.45 1.41-.08Zm11.48 1.88c.18-.19.52-.19.7 0 .05.04.09.1.11.16.03.06.04.12.04.19 0 .13-.05.26-.15.35-.09.1-.22.15-.35.15s-.26-.05-.35-.15a.355.355 0 0 1-.11-.16.433.433 0 0 1-.04-.19c0-.13.05-.26.15-.35Zm-4.93-1.86a.75.75 0 1 1 1.059-1.06.75.75 0 0 1-1.059 1.06Z"/></svg></button></div><script>(function(win) {
    if (!document.querySelector('.pcb')) {
        return;
    }

    var cbConfig = {
        behaviour: document.querySelector('.pcb').getAttribute('data-behaviour'),
        behaviourLink: document.querySelector('.pcb').getAttribute('data-behaviour-link'),
        revision: document.querySelector('.pcb').getAttribute('data-revision'),
        configTTL: parseInt(document.querySelector('.pcb').getAttribute('data-config-ttl'), 10),
        debugMode: document.querySelector('.pcb').getAttribute('data-debug-mode') === 'true',
        initialState: null,
        initialLsState: null,
        previouslyAccepted: []
    };

    var cbUI = {
        wrapper: document.querySelector('.pcb'),
        banner: {
            element: null,
            btnAccept: null,
            btnReject: null,
            btnConfigure: null
        },
        popup: {
            element: null,
            btnClose: null,
            btnSave: null,
            btnAccept: null,
            btnReject: null,
            checkboxes: null,
        },
        overlay: null,
        badge: null,
        blockedScripts: document.querySelectorAll('script[type^="gdpr-blocker/"]'),
        triggerLinks: cbConfig.behaviourLink ? document.querySelectorAll('a[href*="' + cbConfig.behaviourLink + '"]') : null
    };

    function initUI () {
        // setup banner elements
        cbUI.banner.element = cbUI.wrapper.querySelector('.pcb__banner');
        cbUI.banner.btnAccept = cbUI.banner.element.querySelector('.pcb__btn--accept');
        cbUI.banner.btnReject = cbUI.banner.element.querySelector('.pcb__btn--reject');
        cbUI.banner.btnConfigure = cbUI.banner.element.querySelector('.pcb__btn--configure');

        // setup popup elements
        if (cbUI.wrapper.querySelector('.pcb__popup')) {
            cbUI.popup.element = cbUI.wrapper.querySelector('.pcb__popup');
            cbUI.popup.btnClose = cbUI.wrapper.querySelector('.pcb__popup__close');
            cbUI.popup.btnSave = cbUI.popup.element.querySelector('.pcb__btn--save');
            cbUI.popup.btnAccept = cbUI.popup.element.querySelector('.pcb__btn--accept');
            cbUI.popup.btnReject = cbUI.popup.element.querySelector('.pcb__btn--reject');
            cbUI.popup.checkboxes = cbUI.popup.element.querySelector('input[type="checkbox"]');
            // setup overlay
            cbUI.overlay = cbUI.wrapper.querySelector('.pcb__overlay');
        }

        cbUI.badge = cbUI.wrapper.querySelector('.pcb__badge');

        if (cbConfig.behaviour.indexOf('link') > -1) {
            for (var i = 0; i < cbUI.triggerLinks.length; i++) {
                cbUI.triggerLinks[i].addEventListener('click', function(e) {
                    e.preventDefault();
                    showBannerOrPopup();
                });
            }
        }
    }

    function initState () {
        var lsKeyName = getConfigName();
        var currentConfig = localStorage.getItem(lsKeyName);
        var configIsFresh = checkIfConfigIsFresh();

        if (!configIsFresh || currentConfig === null) {
            if (cbConfig.debugMode) {
                console.log('üç™ Config not found, or configuration expired');
            }

            if (window.publiiCBGCM) {
                gtag('consent', 'default', {
                    'ad_storage': window.publiiCBGCM.defaultState.ad_storage ? 'granted' : 'denied',
                    'ad_personalization': window.publiiCBGCM.defaultState.ad_personalization ? 'granted' : 'denied',
                    'ad_user_data': window.publiiCBGCM.defaultState.ad_user_data ? 'granted' : 'denied',
                    'analytics_storage': window.publiiCBGCM.defaultState.analytics_storage ? 'granted' : 'denied',
                    'personalization_storage': window.publiiCBGCM.defaultState.personalization_storage ? 'granted' : 'denied',
                    'functionality_storage': window.publiiCBGCM.defaultState.functionality_storage ? 'granted' : 'denied',
                    'security_storage': window.publiiCBGCM.defaultState.security_storage ? 'granted' : 'denied'
                });  
                
                if (cbConfig.debugMode) {
                    console.log('üç™ GCMv2 DEFAULT STATE: ' + JSON.stringify({
                        'ad_storage': window.publiiCBGCM.defaultState.ad_storage ? 'granted' : 'denied',
                        'ad_personalization': window.publiiCBGCM.defaultState.ad_personalization ? 'granted' : 'denied',
                        'ad_user_data': window.publiiCBGCM.defaultState.ad_user_data ? 'granted' : 'denied',
                        'analytics_storage': window.publiiCBGCM.defaultState.analytics_storage ? 'granted' : 'denied',
                        'personalization_storage': window.publiiCBGCM.defaultState.personalization_storage ? 'granted' : 'denied',
                        'functionality_storage': window.publiiCBGCM.defaultState.functionality_storage ? 'granted' : 'denied',
                        'security_storage': window.publiiCBGCM.defaultState.security_storage ? 'granted' : 'denied'
                    }));
                }
            }

            showBanner();
        } else if (typeof currentConfig === 'string') {
            if (cbConfig.debugMode) {
                console.log('üç™ Config founded');
            }

            cbConfig.initialLsState = currentConfig.split(',');

            if (window.publiiCBGCM) {
                gtag('consent', 'default', {
                    'ad_storage': getDefaultConsentState(currentConfig, 'ad_storage'),
                    'ad_personalization': getDefaultConsentState(currentConfig, 'ad_personalization'),
                    'ad_user_data': getDefaultConsentState(currentConfig, 'ad_user_data'),
                    'analytics_storage': getDefaultConsentState(currentConfig, 'analytics_storage'),
                    'personalization_storage': getDefaultConsentState(currentConfig, 'personalization_storage'),
                    'functionality_storage': getDefaultConsentState(currentConfig, 'functionality_storage'),
                    'security_storage': getDefaultConsentState(currentConfig, 'security_storage')
                });
                
                if (cbConfig.debugMode) {
                    console.log('üç™ GCMv2 DEFAULT STATE: ' + JSON.stringify({
                        'ad_storage': getDefaultConsentState(currentConfig, 'ad_storage'),
                        'ad_personalization': getDefaultConsentState(currentConfig, 'ad_personalization'),
                        'ad_user_data': getDefaultConsentState(currentConfig, 'ad_user_data'),
                        'analytics_storage': getDefaultConsentState(currentConfig, 'analytics_storage'),
                        'personalization_storage': getDefaultConsentState(currentConfig, 'personalization_storage'),
                        'functionality_storage': getDefaultConsentState(currentConfig, 'functionality_storage'),
                        'security_storage': getDefaultConsentState(currentConfig, 'security_storage')
                    }));
                }
            }

            showBadge();

            if (cbUI.popup.element) {
                var allowedGroups = currentConfig.split(',');
                var checkedCheckboxes = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');

                for (var j = 0; j < checkedCheckboxes.length; j++) {
                    var name = checkedCheckboxes[j].getAttribute('data-group-name');

                    if (name && name !== '-' && allowedGroups.indexOf(name) === -1) {
                        checkedCheckboxes[j].checked = false;
                    }
                }

                for (var i = 0; i < allowedGroups.length; i++) {
                    var checkbox = cbUI.popup.element.querySelector('input[type="checkbox"][data-group-name="' + allowedGroups[i] + '"]');

                    if (checkbox) {
                        checkbox.checked = true;
                    }

                    allowCookieGroup(allowedGroups[i]);
                }
            }
        }

        setTimeout(function () {
            cbConfig.initialState = getInitialStateOfConsents();
        }, 0);
    }

    function checkIfConfigIsFresh () {
        var lastConfigSave = localStorage.getItem('publii-gdpr-cookies-config-save-date');

        if (lastConfigSave === null) {
            return false;
        }

        lastConfigSave = parseInt(lastConfigSave, 10);

        if (lastConfigSave === 0) {
            return true;
        }

        if (+new Date() - lastConfigSave < cbConfig.configTTL * 24 * 60 * 60 * 1000) {
            return true;
        }

        return false;
    }

    function getDefaultConsentState (currentConfig, consentGroup) {
        let configGroups = currentConfig.split(',');

        for (let i = 0; i < configGroups.length; i++) {
            let groupName = configGroups[i];
            let group = window.publiiCBGCM.groups.find(group => group.cookieGroup === groupName);

            if (group && group[consentGroup]) {
                return 'granted';
            }
        }  
        
        if (window.publiiCBGCM.defaultState[consentGroup]) {
            return 'granted'; 
        }
        
        return 'denied';
    }

    function initBannerEvents () {
        cbUI.banner.btnAccept.addEventListener('click', function (e) {
            e.preventDefault();
            acceptAllCookies('banner');
            showBadge();
        }, false);

        if (cbUI.banner.btnReject) {
            cbUI.banner.btnReject.addEventListener('click', function (e) {
                e.preventDefault();
                rejectAllCookies();
                showBadge();
            }, false);
        }

        if (cbUI.banner.btnConfigure) {
            cbUI.banner.btnConfigure.addEventListener('click', function (e) {
                e.preventDefault();
                hideBanner();
                showAdvancedPopup();
                showBadge();
            }, false);
        }
    }

    function initPopupEvents () {
        if (!cbUI.popup.element) {
            return;
        }

        cbUI.overlay.addEventListener('click', function (e) {
            hideAdvancedPopup();
        }, false);

        cbUI.popup.element.addEventListener('click', function (e) {
            e.stopPropagation();
        }, false);

        cbUI.popup.btnAccept.addEventListener('click', function (e) {
            e.preventDefault();
            acceptAllCookies('popup');
        }, false);

        cbUI.popup.btnReject.addEventListener('click', function (e) {
            e.preventDefault();
            rejectAllCookies();
        }, false);

        cbUI.popup.btnSave.addEventListener('click', function (e) {
            e.preventDefault();
            saveConfiguration();
        }, false);

        cbUI.popup.btnClose.addEventListener('click', function (e) {
            e.preventDefault();
            hideAdvancedPopup();
        }, false);
    }

    function initBadgeEvents () {
        if (!cbUI.badge) {
            return;
        }

        cbUI.badge.addEventListener('click', function (e) {
            showBannerOrPopup();
        }, false);
    }

    initUI();
    initState();
    initBannerEvents();
    initPopupEvents();
    initBadgeEvents();

    /**
     * API
     */
    function addScript (src, inline) {
        var newScript = document.createElement('script');

        if (src) {
            newScript.setAttribute('src', src);
        }

        if (inline) {
            newScript.text = inline;
        }

        document.body.appendChild(newScript);
    }

    function allowCookieGroup (allowedGroup) {
        var scripts = document.querySelectorAll('script[type="gdpr-blocker/' + allowedGroup + '"]');
        cbConfig.previouslyAccepted.push(allowedGroup);
    
        for (var j = 0; j < scripts.length; j++) {
            addScript(scripts[j].src, scripts[j].text);
        }

        var groupEvent = new Event('publii-cookie-banner-unblock-' + allowedGroup);
        document.body.dispatchEvent(groupEvent);
        unlockEmbeds(allowedGroup);

        if (cbConfig.debugMode) {
            console.log('üç™ Allowed group: ' + allowedGroup);
        }

        if (window.publiiCBGCM && cbConfig.initialLsState.indexOf(allowedGroup) === -1) {
            let consentResult = {};
            let group = window.publiiCBGCM.groups.find(group => group.cookieGroup === allowedGroup);

            if (group) {
                let foundSomeConsents = false;

                Object.keys(group).forEach(key => {
                    if (key !== 'cookieGroup' && group[key] === true) {
                        consentResult[key] = 'granted';
                        foundSomeConsents = true;
                    }
                });

                if (foundSomeConsents) {
                    gtag('consent', 'update', consentResult);   

                    if (cbConfig.debugMode) {
                        console.log('üç™ GCMv2 UPDATE: ' + JSON.stringify(consentResult));
                    }
                }
            }
        }
    }

    function showBannerOrPopup () {
        if (cbUI.popup.element) {
            showAdvancedPopup();
        } else {
            showBanner();
        }
    }

    function showAdvancedPopup () {
        cbUI.popup.element.classList.add('is-visible');
        cbUI.overlay.classList.add('is-visible');
        cbUI.popup.element.setAttribute('aria-hidden', 'false');
        cbUI.overlay.setAttribute('aria-hidden', 'false');
    }

    function hideAdvancedPopup () {
        cbUI.popup.element.classList.remove('is-visible');
        cbUI.overlay.classList.remove('is-visible');
        cbUI.popup.element.setAttribute('aria-hidden', 'true');
        cbUI.overlay.setAttribute('aria-hidden', 'true');
    }

    function showBanner () {
        cbUI.banner.element.classList.add('is-visible');
        cbUI.banner.element.setAttribute('aria-hidden', 'false');
    }

    function hideBanner () {
        cbUI.banner.element.classList.remove('is-visible');
        cbUI.banner.element.setAttribute('aria-hidden', 'true');
    }

    function showBadge () {
        if (!cbUI.badge) {
            return;
        }

        cbUI.badge.classList.add('is-visible');
        cbUI.badge.setAttribute('aria-hidden', 'false');
    }

    function getConfigName () {
        var lsKeyName = 'publii-gdpr-allowed-cookies';

        if (cbConfig.revision) {
            lsKeyName = lsKeyName + '-v' + parseInt(cbConfig.revision, 10);
        }

        return lsKeyName;
    }

    function storeConfiguration (allowedGroups) {
        var lsKeyName = getConfigName();
        var dataToStore = allowedGroups.join(',');
        localStorage.setItem(lsKeyName, dataToStore);

        if (cbConfig.configTTL === 0) {
            localStorage.setItem('publii-gdpr-cookies-config-save-date', 0);

            if (cbConfig.debugMode) {
                console.log('üç™ Store never expiring configuration');
            }
        } else {
            localStorage.setItem('publii-gdpr-cookies-config-save-date', +new Date());
        }
    }

    function getInitialStateOfConsents () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        if (cbConfig.debugMode) {
            console.log('üç™ Initial state: ' + groups.join(', '));
        }

        return groups;
    }

    function getCurrentStateOfConsents () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        if (cbConfig.debugMode) {
            console.log('üç™ State to save: ' + groups.join(', '));
        }

        return groups;
    }

    function getAllGroups () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        return groups;
    }

    function acceptAllCookies (source) {
        var groupsToAccept = getAllGroups();
        storeConfiguration(groupsToAccept);

        for (var i = 0; i < groupsToAccept.length; i++) {
            var group = groupsToAccept[i];

            if (cbConfig.initialState.indexOf(group) > -1 || cbConfig.previouslyAccepted.indexOf(group) > -1) {
                if (cbConfig.debugMode) {
                    console.log('üç™ Skip previously activated group: ' + group);
                }

                continue;
            }

            allowCookieGroup(group);
        }

        if (cbUI.popup.element) {
            var checkboxesToCheck = cbUI.popup.element.querySelectorAll('input[type="checkbox"]');

            for (var j = 0; j < checkboxesToCheck.length; j++) {
                checkboxesToCheck[j].checked = true;
            }
        }

        if (cbConfig.debugMode) {
            console.log('üç™ Accept all cookies: ', groupsToAccept.join(', '));
        }

        if (source === 'popup') {
            hideAdvancedPopup();
        } else if (source === 'banner') {
            hideBanner();
        }
    }

    function rejectAllCookies () {
        if (cbConfig.debugMode) {
            console.log('üç™ Reject all cookies');
        }

        storeConfiguration([]);
        setTimeout(function () {
            window.location.reload();
        }, 100);
    }

    function saveConfiguration () {
        var groupsToAccept = getCurrentStateOfConsents();
        storeConfiguration(groupsToAccept);

        if (cbConfig.debugMode) {
            console.log('üç™ Save new config: ', groupsToAccept.join(', '));
        }

        if (reloadIsNeeded(groupsToAccept)) {
            setTimeout(function () {
                window.location.reload();
            }, 100);
            return;
        }

        for (var i = 0; i < groupsToAccept.length; i++) {
            var group = groupsToAccept[i];

            if (cbConfig.initialState.indexOf(group) > -1 || cbConfig.previouslyAccepted.indexOf(group) > -1) {
                if (cbConfig.debugMode) {
                    console.log('üç™ Skip previously activated group: ' + group);
                }

                continue;
            }

            allowCookieGroup(group);
        }

        hideAdvancedPopup();
    }

    function reloadIsNeeded (groupsToAccept) {
        // check if user rejected consent for initial groups
        var initialGroups = cbConfig.initialState;
        var previouslyAcceptedGroups = cbConfig.previouslyAccepted;
        var groupsToCheck = initialGroups.concat(previouslyAcceptedGroups);

        for (var i = 0; i < groupsToCheck.length; i++) {
            var groupToCheck = groupsToCheck[i];

            if (groupToCheck !== '' && groupsToAccept.indexOf(groupToCheck) === -1) {
                if (cbConfig.debugMode) {
                    console.log('üç™ Reload is needed due lack of: ', groupToCheck);
                }

                return true;
            }
        }

        return false;
    }

    function unlockEmbeds (cookieGroup) {
        var iframesToUnlock = document.querySelectorAll('.pec-wrapper[data-consent-group-id="' + cookieGroup + '"]');

        for (var i = 0; i < iframesToUnlock.length; i++) {
            var iframeWrapper = iframesToUnlock[i];
            iframeWrapper.querySelector('.pec-overlay').classList.remove('is-active');
            iframeWrapper.querySelector('.pec-overlay').setAttribute('aria-hidden', 'true');
            var iframe = iframeWrapper.querySelector('iframe');
            iframe.setAttribute('src', iframe.getAttribute('data-consent-src'));
        }
    }

    win.publiiEmbedConsentGiven = function (cookieGroup) {
        // it will unlock embeds
        allowCookieGroup(cookieGroup);

        var checkbox = cbUI.popup.element.querySelector('input[type="checkbox"][data-group-name="' + cookieGroup + '"]');

        if (checkbox) {
            checkbox.checked = true;
        }

        var groupsToAccept = getCurrentStateOfConsents();
        storeConfiguration(groupsToAccept);

        if (cbConfig.debugMode) {
            console.log('üç™ Save new config: ', groupsToAccept.join(', '));
        }
    }
})(window);</script></body></html>